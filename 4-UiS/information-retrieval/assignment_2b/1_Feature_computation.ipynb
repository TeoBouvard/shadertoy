{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assignment 2B: Feature computation\n",
    "\n",
    "The purpose of this notebook is to perform the computation of features. \n",
    "\n",
    "Note that some features might be expensive, so you don't want to keep re-computing them. Instead, aim for writing a set of relatively simple feature extractors, each computing one or multiple features, and save their output to separate files. Then, load the pre-computed features from multiple files in the learning step (in the [ranking notebook](2_Ranking.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "SEARCH_URL = 'http://gustav1.ux.uis.no:5002/clueweb12b/_search?q=' #q=united+states&df=title&size=20\n",
    "SEARCH_ANCHORS_URL = 'http://gustav1.ux.uis.no:5002/clueweb12b_anchors/_search?q=' #united+states&df=anchors&size=20\n",
    "TERMVECTORS_URL = 'http://gustav1.ux.uis.no:5002/clueweb12b/' #doc_id/_termvectors?term_statistics=true\n",
    "EXISTS_URL = 'http://gustav1.ux.uis.no:5002/clueweb12b/' # doc_id/_exists\n",
    "ANALYZE_URL = 'http://gustav1.ux.uis.no:5002/clueweb12b/_analyze?' #World%27s+biggest+dog"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_length(query):\n",
    "    \"\"\"Number of terms in the query\"\"\" \n",
    "    return len(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_length(doc_id, documents_stats, field=None):\n",
    "    \"\"\"Length of the document, divided by longest document in collection\"\"\"\n",
    "    try:\n",
    "        length = documents_stats.loc[doc_id, field]['Field_length']\n",
    "        return length     \n",
    "    except KeyError:\n",
    "        #print(f'{doc_id} is not in index')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pagerank(doc_id, pagerank):\n",
    "    \"\"\"Pagerank score of a given document\"\"\"\n",
    "    return pagerank.loc[doc_id]['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_query_coverage(query, doc_id, field, docterms, normalized=False):\n",
    "    score = 0\n",
    "\n",
    "    for term in query:\n",
    "        try:\n",
    "            tf = docterms.loc[doc_id, field, term]['TermFreq'].values[0]\n",
    "            score += 1\n",
    "        except KeyError:\n",
    "            score += 0\n",
    "\n",
    "    if normalized == True:\n",
    "        score /= len(query)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_tf(query, doc_id, field, docterms, docs, strategy='sum', normalized=False):\n",
    "    scores = []\n",
    "\n",
    "    for term in query:\n",
    "        try:\n",
    "            tf = docterms.loc[doc_id, field, term]['TermFreq'].values[0]\n",
    "            scores.append(tf)\n",
    "        except KeyError:\n",
    "            scores.append(0)\n",
    "\n",
    "    if strategy == 'sum':\n",
    "        score = sum(scores)\n",
    "    elif strategy == 'mean':\n",
    "        score = mean(scores)\n",
    "    elif strategy == 'min':\n",
    "        score = min(scores)\n",
    "    elif strategy == 'max':\n",
    "        score = max(scores)\n",
    "\n",
    "    if normalized == True and score != 0:\n",
    "        score /= docs.loc[doc_id, field]['Field_length']\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_tfidf(query, doc_id, field, docterms, docs, strategy='sum', normalized=False):\n",
    "    scores = []\n",
    "\n",
    "    for term in query:\n",
    "        try:\n",
    "            tf = docterms.loc[doc_id, field, term]['TermFreq'].values[0]\n",
    "            idf = docterms.loc[doc_id, field, term]['IDF'].values[0]\n",
    "            scores.append(tf * idf)\n",
    "        except KeyError:\n",
    "            scores.append(0)\n",
    "\n",
    "    if strategy == 'sum':\n",
    "        score = sum(scores)\n",
    "    elif strategy == 'mean':\n",
    "        score = mean(scores)\n",
    "    elif strategy == 'min':\n",
    "        score = min(scores)\n",
    "    elif strategy == 'max':\n",
    "        score = max(scores)\n",
    "\n",
    "    if normalized == True and score != 0:\n",
    "        score /= docs.loc[doc_id, field]['Field_length']\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_bm25(query, doc_id, field, docterms, docs, collection, k1=1.2, b=0.75):\n",
    "    \"\"\"BM25 score on a given field\"\"\"\n",
    "    score = 0\n",
    "\n",
    "    try:\n",
    "        average_field_length = collection.loc[field]['Field_length'] / collection.loc[field]['Num_docs']\n",
    "        document_field_length = docs.loc[doc_id, field]['Field_length']\n",
    "        length_ratio = document_field_length / average_field_length\n",
    "        normalization = k1 * (1 - b + b * length_ratio)\n",
    "    except KeyError:\n",
    "        #print(f'{doc_id}-{field} not in index')\n",
    "        return 0\n",
    "\n",
    "    for term in query:\n",
    "        try:\n",
    "            tf = docterms.loc[doc_id, field, term]['TermFreq'].values[0]\n",
    "            idf = docterms.loc[doc_id, field, term]['IDF'].values[0]\n",
    "            score += idf * (tf * (1 + k1)) / (tf + normalization)\n",
    "        except KeyError:\n",
    "            #print(f'{term} not in {doc_id}-{field}')\n",
    "            score += 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_lm(query, doc_id, field, docterms, docs, terms, collection, lambda_param=0.75):\n",
    "    \"\"\"Language Model score on a given field\"\"\"\n",
    "    score = 0\n",
    "\n",
    "    try:\n",
    "        document_field_length = docs.loc[doc_id, field]['Field_length']\n",
    "        collection_field_length = collection.loc[field]['Field_length']\n",
    "        \n",
    "    except KeyError:\n",
    "        #print(f'{doc_id}-{field} not in index')\n",
    "        return 0\n",
    "\n",
    "    for term in query:\n",
    "        try:\n",
    "            tf = docterms.loc[doc_id, field, term]['TermFreq'].values[0]\n",
    "        except KeyError:\n",
    "            tf = 0\n",
    "\n",
    "        sum_tf = terms.loc[term, field]['SumTermFreq']\n",
    "\n",
    "        ptd = tf / document_field_length\n",
    "        ptc = sum_tf / collection_field_length\n",
    "\n",
    "        raw_score = (1 - lambda_param) * ptd + lambda_param * ptc\n",
    "        score += math.log(raw_score) if raw_score > 0 else 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "## File loading utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(q):\n",
    "    params = urlencode({'text': q})\n",
    "    query = f'{ANALYZE_URL}{params}'\n",
    "    response = json.loads(requests.get(query).text)\n",
    "    if response != {}:\n",
    "        processed_query = [item['token'] for item in response['tokens']]\n",
    "    else:\n",
    "        raise ValueError(f'Query {q} could not be processed')\n",
    "    return processed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(path):\n",
    "    with open(path) as f:\n",
    "        query_list = f.readlines()\n",
    "    queries = {q.split()[0]:process_query(' '.join(q.split()[1:])) for q in query_list} \n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_in_index(doc_id):\n",
    "    query = f'{EXISTS_URL}{doc_id}/_exists'\n",
    "    response = json.loads(requests.get(query).text)\n",
    "    return response['exists']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(query, q_id, ranking):\n",
    "    \"\"\" Retrieve all documents in the fist pass ranking for a given query \"\"\"\n",
    "    documents = ranking.loc[q_id]['DocumentId'].to_list()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading queries, qrels, and first pass ranking\n",
    "train_queries = load_queries('data/queries.txt')\n",
    "test_queries = load_queries('data/queries2.txt')\n",
    "\n",
    "train_qrels = pd.read_csv('data/qrels.csv')\n",
    "train_qrels.set_index('QueryId', inplace=True)\n",
    "\n",
    "train_first_pass = pd.read_csv('data/ranking_bm25.csv')\n",
    "train_first_pass.set_index('QueryId', inplace=True)\n",
    "\n",
    "test_first_pass = pd.read_csv('data/ranking2_bm25.csv')\n",
    "test_first_pass.set_index('QueryId', inplace=True)\n",
    "\n",
    "# Loading collection statistics\n",
    "terms_stats = pd.read_csv('data/stats_terms.tsv', sep='\\t')\n",
    "terms_stats.set_index(['Term', 'Field'], inplace=True)\n",
    "\n",
    "collection_stats = pd.read_csv('data/stats_coll.tsv', sep='\\t')\n",
    "collection_stats.set_index('Field', inplace=True)\n",
    "\n",
    "docs_stats = pd.read_csv('data/stats_docs.tsv', sep='\\t')\n",
    "docs_stats.set_index(['DocumentId', 'Field'], inplace=True)\n",
    "\n",
    "docterms_stats = pd.read_csv('data/stats_docs_terms.tsv', sep='\\t')\n",
    "docterms_stats.set_index(['DocumentId', 'Field', 'Term'], inplace=True)\n",
    "\n",
    "# Loading pagerank\n",
    "pagerank = pd.read_csv('data/pagerank.csv', sep=' ', header=None, names=['DocumentId', 'Score'], index_col='DocumentId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "## Feature computation\n",
    "\n",
    "For each feature extractor above, we compute the features on the documents retrieved from the first pass retrieval. Features are computed for both train and test sets, and saved to files in order to be loaded in the ranking part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(mode='train'):\n",
    "    \n",
    "    if mode == 'train':\n",
    "        queries=train_queries\n",
    "        first_pass=train_first_pass\n",
    "    else:\n",
    "        queries=test_queries\n",
    "        first_pass=test_first_pass\n",
    "        \n",
    "    features = []\n",
    "    \n",
    "    for q_id, query in tqdm(queries.items()):\n",
    "\n",
    "        docs = load_documents(query, q_id, first_pass)\n",
    "\n",
    "        for d in docs:\n",
    "            \n",
    "            # Field length \n",
    "            #f = dict(QueryId=q_id, \n",
    "            #         DocumentId=d,\n",
    "            #         content_length=field_length(d, docs_stats, 'content'),\n",
    "            #         title_length=field_length(d, docs_stats, 'title'),\n",
    "            #         anchors_length=field_length(d, docs_stats, 'anchors'))\n",
    "\n",
    "            # Query length \n",
    "            #f = dict(QueryId=q_id, \n",
    "            #         DocumentId=d,\n",
    "            #         query_length=query_length(query))\n",
    "\n",
    "            # Term frequency\n",
    "            #f = dict(QueryId=q_id, \n",
    "            #         DocumentId=d,\n",
    "            #         TF_title_sum=feature_tf(query, d, 'title', docterms_stats, docs_stats, strategy='sum', normalized=False),\n",
    "            #         TF_content_sum=feature_tf(query, d, 'content', docterms_stats, docs_stats, strategy='sum', normalized=False),\n",
    "            #         TF_anchors_sum=feature_tf(query, d, 'anchors', docterms_stats, docs_stats, strategy='sum', normalized=False),\n",
    "\n",
    "            #         TF_title_mean=feature_tf(query, d, 'title', docterms_stats, docs_stats, strategy='mean', normalized=False),\n",
    "            #         TF_content_mean=feature_tf(query, d, 'content', docterms_stats, docs_stats, strategy='mean', normalized=False),\n",
    "            #         TF_anchors_mean=feature_tf(query, d, 'anchors', docterms_stats, docs_stats, strategy='mean', normalized=False),\n",
    "\n",
    "            #         TF_title_max=feature_tf(query, d, 'title', docterms_stats, docs_stats, strategy='max', normalized=False),\n",
    "            #         TF_content_max=feature_tf(query, d, 'content', docterms_stats, docs_stats, strategy='max', normalized=False),\n",
    "            #         TF_anchors_max=feature_tf(query, d, 'anchors', docterms_stats, docs_stats, strategy='max', normalized=False),\n",
    "\n",
    "            #         TF_title_min=feature_tf(query, d, 'title', docterms_stats, docs_stats, strategy='min', normalized=False),\n",
    "            #         TF_content_min=feature_tf(query, d, 'content', docterms_stats, docs_stats, strategy='min', normalized=False),\n",
    "            #         TF_anchors_min=feature_tf(query, d, 'anchors', docterms_stats, docs_stats, strategy='min', normalized=False),\n",
    "\n",
    "            #         normalized_TF_title_sum=feature_tf(query, d, 'title', docterms_stats, docs_stats, strategy='sum', normalized=True),\n",
    "            #         normalized_TF_content_sum=feature_tf(query, d, 'content', docterms_stats, docs_stats, strategy='sum', normalized=True),\n",
    "            #         normalized_TF_anchors_sum=feature_tf(query, d, 'anchors', docterms_stats, docs_stats, strategy='sum', normalized=True),\n",
    "\n",
    "            #         normalized_TF_title_mean=feature_tf(query, d, 'title', docterms_stats, docs_stats, strategy='mean', normalized=True),\n",
    "            #         normalized_TF_content_mean=feature_tf(query, d, 'content', docterms_stats, docs_stats, strategy='mean', normalized=True),\n",
    "            #         normalized_TF_anchors_mean=feature_tf(query, d, 'anchors', docterms_stats, docs_stats, strategy='mean', normalized=True),\n",
    "\n",
    "            #         normalized_TF_title_max=feature_tf(query, d, 'title', docterms_stats, docs_stats, strategy='max', normalized=True),\n",
    "            #         normalized_TF_content_max=feature_tf(query, d, 'content', docterms_stats, docs_stats, strategy='max', normalized=True),\n",
    "            #         normalized_TF_anchors_max=feature_tf(query, d, 'anchors', docterms_stats, docs_stats, strategy='max', normalized=True),\n",
    "\n",
    "            #         normalized_TF_title_min=feature_tf(query, d, 'title', docterms_stats, docs_stats, strategy='min', normalized=True),\n",
    "            #         normalized_TF_content_min=feature_tf(query, d, 'content', docterms_stats, docs_stats, strategy='min', normalized=True),\n",
    "            #         normalized_TF_anchors_min=feature_tf(query, d, 'anchors', docterms_stats, docs_stats, strategy='min', normalized=True))\n",
    "\n",
    "            # TFIDF\n",
    "            #f = dict(QueryId=q_id, \n",
    "            #         DocumentId=d,\n",
    "            #         TFIDF_title_sum=feature_tfidf(query, d, 'title', docterms_stats, docs_stats, strategy='sum', normalized=False),\n",
    "            #         TFIDF_content_sum=feature_tfidf(query, d, 'content', docterms_stats, docs_stats, strategy='sum', normalized=False),\n",
    "            #         TFIDF_anchors_sum=feature_tfidf(query, d, 'anchors', docterms_stats, docs_stats, strategy='sum', normalized=False),\n",
    "\n",
    "            #         TFIDF_title_mean=feature_tfidf(query, d, 'title', docterms_stats, docs_stats, strategy='mean', normalized=False),\n",
    "            #         TFIDF_content_mean=feature_tfidf(query, d, 'content', docterms_stats, docs_stats, strategy='mean', normalized=False),\n",
    "            #         TFIDF_anchors_mean=feature_tfidf(query, d, 'anchors', docterms_stats, docs_stats, strategy='mean', normalized=False),\n",
    "\n",
    "            #         TFIDF_title_max=feature_tfidf(query, d, 'title', docterms_stats, docs_stats, strategy='max', normalized=False),\n",
    "            #         TFIDF_content_max=feature_tfidf(query, d, 'content', docterms_stats, docs_stats, strategy='max', normalized=False),\n",
    "            #         TFIDF_anchors_max=feature_tfidf(query, d, 'anchors', docterms_stats, docs_stats, strategy='max', normalized=False),\n",
    "\n",
    "            #         TFIDF_title_min=feature_tfidf(query, d, 'title', docterms_stats, docs_stats, strategy='min', normalized=False),\n",
    "            #         TFIDF_content_min=feature_tfidf(query, d, 'content', docterms_stats, docs_stats, strategy='min', normalized=False),\n",
    "            #         TFIDF_anchors_min=feature_tfidf(query, d, 'anchors', docterms_stats, docs_stats, strategy='min', normalized=False),\n",
    "\n",
    "            #         normalized_TFIDF_title_sum=feature_tfidf(query, d, 'title', docterms_stats, docs_stats, strategy='sum', normalized=True),\n",
    "            #         normalized_TFIDF_content_sum=feature_tfidf(query, d, 'content', docterms_stats, docs_stats, strategy='sum', normalized=True),\n",
    "            #         normalized_TFIDF_anchors_sum=feature_tfidf(query, d, 'anchors', docterms_stats, docs_stats, strategy='sum', normalized=True),\n",
    "\n",
    "            #         normalized_TFIDF_title_mean=feature_tfidf(query, d, 'title', docterms_stats, docs_stats, strategy='mean', normalized=True),\n",
    "            #         normalized_TFIDF_content_mean=feature_tfidf(query, d, 'content', docterms_stats, docs_stats, strategy='mean', normalized=True),\n",
    "            #         normalized_TFIDF_anchors_mean=feature_tfidf(query, d, 'anchors', docterms_stats, docs_stats, strategy='mean', normalized=True),\n",
    "\n",
    "            #         normalized_TFIDF_title_max=feature_tfidf(query, d, 'title', docterms_stats, docs_stats, strategy='max', normalized=True),\n",
    "            #         normalized_TFIDF_content_max=feature_tfidf(query, d, 'content', docterms_stats, docs_stats, strategy='max', normalized=True),\n",
    "            #         normalized_TFIDF_anchors_max=feature_tfidf(query, d, 'anchors', docterms_stats, docs_stats, strategy='max', normalized=True),\n",
    "\n",
    "            #         normalized_TFIDF_title_min=feature_tfidf(query, d, 'title', docterms_stats, docs_stats, strategy='min', normalized=True),\n",
    "            #         normalized_TFIDF_content_min=feature_tfidf(query, d, 'content', docterms_stats, docs_stats, strategy='min', normalized=True),\n",
    "            #         normalized_TFIDF_anchors_min=feature_tfidf(query, d, 'anchors', docterms_stats, docs_stats, strategy='min', normalized=True))\n",
    "\n",
    "            # Query coverage\n",
    "            f = dict(QueryId=q_id,\n",
    "                     DocumentId=d,\n",
    "                     title_query_coverage=feature_query_coverage(query, d, 'title', docterms_stats, normalized=False),\n",
    "                     content_query_coverage=feature_query_coverage(query, d, 'content', docterms_stats, normalized=False),\n",
    "                     anchors_query_coverage=feature_query_coverage(query, d, 'anchors', docterms_stats, normalized=False),\n",
    "\n",
    "                     normalized_title_query_coverage=feature_query_coverage(query, d, 'title', docterms_stats, normalized=True),\n",
    "                     normalized_content_query_coverage=feature_query_coverage(query, d, 'content', docterms_stats, normalized=True),\n",
    "                     normalized_anchors_query_coverage=feature_query_coverage(query, d, 'anchors', docterms_stats, normalized=True))\n",
    "\n",
    "            # PageRank\n",
    "            #f = dict(QueryId=q_id,\n",
    "            #         DocumentId=d,\n",
    "            #         pagerank_score=feature_pagerank(d, pagerank))\n",
    "\n",
    "            # BM25 \n",
    "            #f = dict(QueryId=q_id, \n",
    "            #         DocumentId=d,\n",
    "            #         bm25_content=feature_bm25(query, d, 'content', docterms_stats, docs_stats, collection_stats),\n",
    "            #         bm25_title=feature_bm25(query, d, 'title', docterms_stats, docs_stats, collection_stats), \n",
    "            #         bm25_anchors=feature_bm25(query, d, 'anchors', docterms_stats, docs_stats, collection_stats))\n",
    "\n",
    "            # LM\n",
    "            #f = dict(QueryId=q_id, \n",
    "            #         DocumentId=d,\n",
    "            #         lm_content=feature_lm(query, d, 'content', docterms_stats, docs_stats, terms_stats, collection_stats),\n",
    "            #         lm_title=feature_lm(query, d, 'title', docterms_stats, docs_stats, terms_stats, collection_stats), \n",
    "            #         lm_anchors=feature_lm(query, d, 'anchors', docterms_stats, docs_stats, terms_stats, collection_stats))\n",
    "\n",
    "            features.append(f)\n",
    "\n",
    "    features = pd.DataFrame.from_dict(features).set_index(['QueryId', 'DocumentId'])\n",
    "\n",
    "    # write computed features to file\n",
    "    features.to_csv(f'data/{mode}_features_qcoverage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "0%|          | 0/50 [00:00<?, ?it/s]/home/arthurdent/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: PerformanceWarning: indexing past lexsort depth may impact performance.\n  \n100%|██████████| 50/50 [00:25<00:00,  1.92it/s]\n100%|██████████| 50/50 [00:24<00:00,  2.04it/s]\n"
    }
   ],
   "source": [
    "compute_features(mode='train')\n",
    "compute_features(mode='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}