{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A, Part 2: Retrieval\n",
    "\n",
    "Implement BM25 and LM retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hashedindex import textparser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "N_GRAMS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FILE = 'data/queries.txt'  # make sure the query file exists on this location\n",
    "BM25_OUTPUT_FILE = 'data/bm25_singlefield.txt'  # output the BM25 ranking\n",
    "LM_OUTPUT_FILE = 'data/lm_singlefield.txt'  # output the LM ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = load_file('data/basic_index_new_idf.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('los',),\n",
       " ('angeles',),\n",
       " ('every',),\n",
       " ('year',),\n",
       " ('millions',),\n",
       " ('americans',),\n",
       " ('pledge',),\n",
       " ('put',),\n",
       " ('financial',),\n",
       " ('house',),\n",
       " ('order',),\n",
       " ('say',),\n",
       " ('different',),\n",
       " ('theyll',),\n",
       " ('save',),\n",
       " ('invest',),\n",
       " ('even',),\n",
       " ('stick',),\n",
       " ('budget',),\n",
       " ('indeed',),\n",
       " ('second',),\n",
       " ('popular',),\n",
       " ('new',),\n",
       " ('resolution',),\n",
       " ('achieve',),\n",
       " ('goals',),\n",
       " ('according',),\n",
       " ('citibank',),\n",
       " ('first',),\n",
       " ('lose',),\n",
       " ('weight',),\n",
       " ('live',),\n",
       " ('healthfully',),\n",
       " ('well',),\n",
       " ('leave',),\n",
       " ('loss',),\n",
       " ('jenny',),\n",
       " ('craig',),\n",
       " ('hard',),\n",
       " ('steel',),\n",
       " ('dramatic',),\n",
       " ('decrease',),\n",
       " ('lifestyle',),\n",
       " ('dont',),\n",
       " ('panic',),\n",
       " ('many',),\n",
       " ('changes',),\n",
       " ('made',),\n",
       " ('without',),\n",
       " ('much',),\n",
       " ('suffering',),\n",
       " ('matter',),\n",
       " ('plugging',),\n",
       " ('money',),\n",
       " ('leaks',),\n",
       " ('life',),\n",
       " ('wont',),\n",
       " ('deflate',),\n",
       " ('italjoie',),\n",
       " ('de',),\n",
       " ('vivreend',),\n",
       " ('ital',),\n",
       " ('keep',),\n",
       " ('mind',),\n",
       " ('saving',),\n",
       " ('end',),\n",
       " ('race',),\n",
       " ('see',),\n",
       " ('one',),\n",
       " ('provide',),\n",
       " ('adequately',),\n",
       " ('future',),\n",
       " ('gives',),\n",
       " ('us',),\n",
       " ('freedom',),\n",
       " ('enjoy',),\n",
       " ('important',),\n",
       " ('things',),\n",
       " ('tips',),\n",
       " ('sisters',),\n",
       " ('savings',),\n",
       " ('mary',),\n",
       " ('hunt',),\n",
       " ('editor',),\n",
       " ('cheapskate',),\n",
       " ('monthly',),\n",
       " ('newsletter',),\n",
       " ('based',),\n",
       " ('paramount',),\n",
       " ('members',),\n",
       " ('beardstown',),\n",
       " ('ladies',),\n",
       " ('investment',),\n",
       " ('club',),\n",
       " ('wrote',),\n",
       " ('smart',),\n",
       " ('spending',),\n",
       " ('big',),\n",
       " ('senior',),\n",
       " ('ill',),\n",
       " ('got',),\n",
       " ('trouble',),\n",
       " ('miscalculating',),\n",
       " ('published',),\n",
       " ('returns',),\n",
       " ('nevertheless',),\n",
       " ('wise',),\n",
       " ('advice',),\n",
       " ('pay',),\n",
       " ('attention',),\n",
       " ('yield',),\n",
       " ('greater',),\n",
       " ('people',),\n",
       " ('spend',),\n",
       " ('hours',),\n",
       " ('clipping',),\n",
       " ('coupons',),\n",
       " ('grocery',),\n",
       " ('bill',),\n",
       " ('often',),\n",
       " ('enough',),\n",
       " ('time',),\n",
       " ('shopping',),\n",
       " ('around',),\n",
       " ('auto',),\n",
       " ('homeowners',),\n",
       " ('insurance',),\n",
       " ('bigger',),\n",
       " ('may',),\n",
       " ('drive',),\n",
       " ('extra',),\n",
       " ('mile',),\n",
       " ('get',),\n",
       " ('soda',),\n",
       " ('cents',),\n",
       " ('cheaper',),\n",
       " ('think',),\n",
       " ('nothing',),\n",
       " ('buying',),\n",
       " ('extended',),\n",
       " ('warranties',),\n",
       " ('cost',),\n",
       " ('lot',),\n",
       " ('unnecessary',),\n",
       " ('focus',),\n",
       " ('cutting',),\n",
       " ('bills',),\n",
       " ('make',),\n",
       " ('payment',),\n",
       " ('mortgage',),\n",
       " ('years',),\n",
       " ('percent',),\n",
       " ('would',),\n",
       " ('increase',),\n",
       " ('paying',),\n",
       " ('reduce',),\n",
       " ('loan',),\n",
       " ('repayment',),\n",
       " ('period',),\n",
       " ('biweekly',),\n",
       " ('option',),\n",
       " ('lenders',),\n",
       " ('charge',),\n",
       " ('switching',),\n",
       " ('schedules',),\n",
       " ('lower',),\n",
       " ('premium',),\n",
       " ('company',),\n",
       " ('smoking',),\n",
       " ('installing',),\n",
       " ('smoke',),\n",
       " ('detectors',),\n",
       " ('fire',),\n",
       " ('extinguishers',),\n",
       " ('resistant',),\n",
       " ('doors',),\n",
       " ('security',),\n",
       " ('alarm',),\n",
       " ('system',),\n",
       " ('could',),\n",
       " ('also',),\n",
       " ('discount',),\n",
       " ('raising',),\n",
       " ('deductible',),\n",
       " ('works',),\n",
       " ('insurers',),\n",
       " ('give',),\n",
       " ('breaks',),\n",
       " ('retirees',),\n",
       " ('longtime',),\n",
       " ('customers',),\n",
       " ('consider',),\n",
       " ('disability',),\n",
       " ('survey',),\n",
       " ('seriously',),\n",
       " ('medical',),\n",
       " ('still',),\n",
       " ('lost',),\n",
       " ('everything',),\n",
       " ('owned',),\n",
       " ('resulting',),\n",
       " ('half',),\n",
       " ('foreclosures',),\n",
       " ('result',),\n",
       " ('far',),\n",
       " ('death',),\n",
       " ('automatically',),\n",
       " ('buy',),\n",
       " ('name',),\n",
       " ('brand',),\n",
       " ('appliance',),\n",
       " ('general',),\n",
       " ('electric',),\n",
       " ('makes',),\n",
       " ('gibson',),\n",
       " ('kelvinator',),\n",
       " ('tappan',),\n",
       " ('white',),\n",
       " ('westinghouse',),\n",
       " ('maytag',),\n",
       " ('admiral',),\n",
       " ('hardwick',),\n",
       " ('magic',),\n",
       " ('chef',),\n",
       " ('jenn',),\n",
       " ('air',),\n",
       " ('montgomery',),\n",
       " ('ward',),\n",
       " ('norge',),\n",
       " ('whirlpool',),\n",
       " ('kitchenaid',),\n",
       " ('roper',),\n",
       " ('estate',),\n",
       " ('sears',),\n",
       " ('appliances',),\n",
       " ('ge',),\n",
       " ('others',),\n",
       " ('energy',),\n",
       " ('efficient',),\n",
       " ('example',),\n",
       " ('gas',),\n",
       " ('ranges',),\n",
       " ('automatic',),\n",
       " ('spark',),\n",
       " ('ignition',),\n",
       " ('ones',),\n",
       " ('pilot',),\n",
       " ('lights',),\n",
       " ('skip',),\n",
       " ('wash',),\n",
       " ('clothes',),\n",
       " ('cold',),\n",
       " ('water',),\n",
       " ('doesnt',),\n",
       " ('difference',),\n",
       " ('saves',),\n",
       " ('heating',),\n",
       " ('temperature',),\n",
       " ('heater',),\n",
       " ('degrees',),\n",
       " ('hot',),\n",
       " ('youll',),\n",
       " ('insulate',),\n",
       " ('home',),\n",
       " ('properly',),\n",
       " ('retain',),\n",
       " ('heat',),\n",
       " ('shade',),\n",
       " ('cool',),\n",
       " ('conditioning',),\n",
       " ('costs',),\n",
       " ('slightly',),\n",
       " ('used',),\n",
       " ('car',),\n",
       " ('cars',),\n",
       " ('depreciate',),\n",
       " ('purchase',),\n",
       " ('longer',),\n",
       " ('maintain',),\n",
       " ('borrow',),\n",
       " ('better',),\n",
       " ('going',),\n",
       " ('debt',),\n",
       " ('nice',),\n",
       " ('shoes',),\n",
       " ('tap',),\n",
       " ('equity',),\n",
       " ('interest',),\n",
       " ('rate',),\n",
       " ('offered',),\n",
       " ('credit',),\n",
       " ('cards',),\n",
       " ('deduct',),\n",
       " ('taxes',),\n",
       " ('contribute',),\n",
       " ('tax',),\n",
       " ('deferred',),\n",
       " ('retirement',),\n",
       " ('plan',),\n",
       " ('401k',),\n",
       " ('ira',),\n",
       " ('card',),\n",
       " ('rebates',),\n",
       " ('airline',),\n",
       " ('miles',),\n",
       " ('perks',),\n",
       " ('travel',),\n",
       " ('season',),\n",
       " ('instead',),\n",
       " ('eating',),\n",
       " ('dinner',),\n",
       " ('friends',),\n",
       " ('try',),\n",
       " ('dessert',),\n",
       " ('fads',),\n",
       " ('fashions',),\n",
       " ('usually',),\n",
       " ('coins',),\n",
       " ('add',),\n",
       " ('hundreds',),\n",
       " ('thousands',),\n",
       " ('dollars',),\n",
       " ('go',),\n",
       " ('afternoon',),\n",
       " ('movie',),\n",
       " ('matinees',),\n",
       " ('third',),\n",
       " ('ticket',),\n",
       " ('bulk',),\n",
       " ('especially',),\n",
       " ('medicine',),\n",
       " ('aspirin',),\n",
       " ('generic',),\n",
       " ('brands',),\n",
       " ('xxx',),\n",
       " ('pasadena',),\n",
       " ('fisherman',),\n",
       " ('story',),\n",
       " ('away',),\n",
       " ('ucla',),\n",
       " ('receiver',),\n",
       " ('brad',),\n",
       " ('melsby',),\n",
       " ('fresh',),\n",
       " ('today',),\n",
       " ('catch',),\n",
       " ('last',),\n",
       " ('month',),\n",
       " ('spot',),\n",
       " ('renowned',),\n",
       " ('dolphins',),\n",
       " ('reeled',),\n",
       " ('thing',),\n",
       " ('beauty',),\n",
       " ('ornery',),\n",
       " ('sucker',),\n",
       " ('hook',),\n",
       " ('line',),\n",
       " ('sinker',),\n",
       " ('released',),\n",
       " ('swept',),\n",
       " ('hurricane',),\n",
       " ('unlike',),\n",
       " ('stories',),\n",
       " ('witnesses',),\n",
       " ('national',),\n",
       " ('television',),\n",
       " ('audience',),\n",
       " ('boot',),\n",
       " ('espn',),\n",
       " ('fishin',),\n",
       " ('hole',),\n",
       " ('happy',),\n",
       " ('report',),\n",
       " ('setback',),\n",
       " ('miami',),\n",
       " ('black',),\n",
       " ('swallowed',),\n",
       " ('whole',),\n",
       " ('spitting',),\n",
       " ('bruins',),\n",
       " ('game',),\n",
       " ('winning',),\n",
       " ('streak',),\n",
       " ('title',),\n",
       " ('hopes',),\n",
       " ('junior',),\n",
       " ('split',),\n",
       " ('bummed',),\n",
       " ('days',),\n",
       " ('soon',),\n",
       " ('realized',),\n",
       " ('fumble',),\n",
       " ('marvelous',),\n",
       " ('late',),\n",
       " ('fourth',),\n",
       " ('quarter',),\n",
       " ('wasnt',),\n",
       " ('sole',),\n",
       " ('reason',),\n",
       " ('deep',),\n",
       " ('sixed',),\n",
       " ('orange',),\n",
       " ('bowl',),\n",
       " ('seemed',),\n",
       " ('way',),\n",
       " ('snippet',),\n",
       " ('sports',),\n",
       " ('highlights',),\n",
       " ('shows',),\n",
       " ('ran',),\n",
       " ('excruciating',),\n",
       " ('watch',),\n",
       " ('football',),\n",
       " ('necessarily',),\n",
       " ('bad',),\n",
       " ('replays',),\n",
       " ('clearly',),\n",
       " ('showed',),\n",
       " ('ball',),\n",
       " ('squirted',),\n",
       " ('free',),\n",
       " ('three',),\n",
       " ('minutes',),\n",
       " ('play',),\n",
       " ('proceeded',),\n",
       " ('yards',),\n",
       " ('points',),\n",
       " ('saw',),\n",
       " ('innocence',),\n",
       " ('upheld',),\n",
       " ('easier',),\n",
       " ('let',),\n",
       " ('personal',),\n",
       " ('guilt',),\n",
       " ('twenty',),\n",
       " ('seven',),\n",
       " ('later',),\n",
       " ('eager',),\n",
       " ('wade',),\n",
       " ('back',),\n",
       " ('current',),\n",
       " ('bait',),\n",
       " ('wisconsin',),\n",
       " ('secondary',),\n",
       " ('land',),\n",
       " ('day',),\n",
       " ('easy',),\n",
       " ('able',),\n",
       " ('oh',),\n",
       " ('goes',),\n",
       " ('said',),\n",
       " ('recent',),\n",
       " ('practice',),\n",
       " ('leading',),\n",
       " ('rose',),\n",
       " ('date',),\n",
       " ('no9',),\n",
       " ('badgers',),\n",
       " ('ever',),\n",
       " ('feel',),\n",
       " ('partly',),\n",
       " ('responsible',),\n",
       " ('happened',),\n",
       " ('chance',),\n",
       " ('take',),\n",
       " ('coming',),\n",
       " ('answer',),\n",
       " ('qualifies',),\n",
       " ('filibuster',),\n",
       " ('knees',),\n",
       " ('ankles',),\n",
       " ('knuckles',),\n",
       " ('locker',),\n",
       " ('room',),\n",
       " ('louder',),\n",
       " ('former',),\n",
       " ('county',),\n",
       " ('prep',),\n",
       " ('standout',),\n",
       " ('quietest',),\n",
       " ('communications',),\n",
       " ('majors',),\n",
       " ('reserving',),\n",
       " ('words',),\n",
       " ('journals',),\n",
       " ('keeps',),\n",
       " ('introversion',),\n",
       " ('mistaken',),\n",
       " ('introspection',),\n",
       " ('though',),\n",
       " ('outsiders',),\n",
       " ('persist',),\n",
       " ('pursing',),\n",
       " ('angle',),\n",
       " ('worst',),\n",
       " ('enemy',),\n",
       " ('exaggerated',),\n",
       " ('bit',),\n",
       " ('redshirt',),\n",
       " ('caught',),\n",
       " ('yard',),\n",
       " ('touchdown',),\n",
       " ('pass',),\n",
       " ('seconds',),\n",
       " ('left',),\n",
       " ('beat',),\n",
       " ('oregon',),\n",
       " ('state',),\n",
       " ('november',),\n",
       " ('im',),\n",
       " ('receivers',),\n",
       " ('hide',),\n",
       " ('disappointed',),\n",
       " ('show',),\n",
       " ('kind',),\n",
       " ('perfectionist',),\n",
       " ('point',),\n",
       " ('self',),\n",
       " ('destructive',),\n",
       " ('offensive',),\n",
       " ('tackle',),\n",
       " ('kris',),\n",
       " ('farris',),\n",
       " ('closest',),\n",
       " ('friend',),\n",
       " ('team',),\n",
       " ('along',),\n",
       " ('defensive',),\n",
       " ('pete',),\n",
       " ('holland',),\n",
       " ('like',),\n",
       " ('oscar',),\n",
       " ('felix',),\n",
       " ('scattering',),\n",
       " ('sentences',),\n",
       " ('place',),\n",
       " ('neatly',),\n",
       " ('compartmentalizing',),\n",
       " ('thoughts',),\n",
       " ('playful',),\n",
       " ('pensive',),\n",
       " ('laconic',),\n",
       " ('loquacious',),\n",
       " ('tick',),\n",
       " ('winners',),\n",
       " ('best',),\n",
       " ('picture',),\n",
       " ('rattle',),\n",
       " ('stanley',),\n",
       " ('cup',),\n",
       " ('champions',),\n",
       " ('theyre',),\n",
       " ('completely',),\n",
       " ('wonder',),\n",
       " ('clicked',),\n",
       " ('start',),\n",
       " ('couldnt',),\n",
       " ('sure',),\n",
       " ('good',),\n",
       " ('player',),\n",
       " ('santa',),\n",
       " ('margarita',),\n",
       " ('high',),\n",
       " ('awe',),\n",
       " ('putting',),\n",
       " ('astonishing',),\n",
       " ('numbers',),\n",
       " ('alamitos',),\n",
       " ('wind',),\n",
       " ('career',),\n",
       " ('school',),\n",
       " ('history',),\n",
       " ('shortly',),\n",
       " ('committed',),\n",
       " ('called',),\n",
       " ('telephone',),\n",
       " ('nervous',),\n",
       " ('recalled',),\n",
       " ('pretty',),\n",
       " ('reading',),\n",
       " ('daily',),\n",
       " ('papers',),\n",
       " ('asked',),\n",
       " ('wanted',),\n",
       " ('roommates',),\n",
       " ('ok',),\n",
       " ('conversation',),\n",
       " ('sort',),\n",
       " ('died',),\n",
       " ('right',),\n",
       " ('idiot',),\n",
       " ('asking',),\n",
       " ('moved',),\n",
       " ('freshman',),\n",
       " ('continued',),\n",
       " ('warming',),\n",
       " ('subject',),\n",
       " ('remember',),\n",
       " ('cracked',),\n",
       " ('open',),\n",
       " ('stephen',),\n",
       " ('king',),\n",
       " ('novel',),\n",
       " ('talk',),\n",
       " ('read',),\n",
       " ('guy',),\n",
       " ('four',),\n",
       " ('changed',),\n",
       " ('cant',),\n",
       " ('crack',),\n",
       " ('book',),\n",
       " ('getting',),\n",
       " ('sidetracked',),\n",
       " ('wisecracks',),\n",
       " ('living',),\n",
       " ('fun',),\n",
       " ('helped',),\n",
       " ('humor',),\n",
       " ('situations',),\n",
       " ('close',),\n",
       " ('single',),\n",
       " ('word',),\n",
       " ('sounds',),\n",
       " ('perfectly',),\n",
       " ('innocuous',),\n",
       " ('anyone',),\n",
       " ('else',),\n",
       " ('hears',),\n",
       " ('send',),\n",
       " ('hysteria',),\n",
       " ('inside',),\n",
       " ('jokes',),\n",
       " ('seinfeld',),\n",
       " ('jerry',),\n",
       " ('george',),\n",
       " ('kramer',),\n",
       " ('sitcom',),\n",
       " ('gig',),\n",
       " ('ending',),\n",
       " ('prematurely',),\n",
       " ('months',),\n",
       " ('uniform',),\n",
       " ('strange',),\n",
       " ('mused',),\n",
       " ('loose',),\n",
       " ('part',),\n",
       " ('mainly',),\n",
       " ('celebrated',),\n",
       " ('lionized',),\n",
       " ('outland',),\n",
       " ('trophy',),\n",
       " ('hearing',),\n",
       " ('siren',),\n",
       " ('song',),\n",
       " ('nfl',),\n",
       " ('meantime',),\n",
       " ('struggled',),\n",
       " ('regain',),\n",
       " ('form',),\n",
       " ('promise',),\n",
       " ('tearing',),\n",
       " ('anterior',),\n",
       " ('cruciate',),\n",
       " ('ligament',),\n",
       " ('knee',),\n",
       " ('spring',),\n",
       " ('drills',),\n",
       " ('sophomore',),\n",
       " ('injury',),\n",
       " ('effectively',),\n",
       " ('two',),\n",
       " ('seasons',),\n",
       " ('physical',),\n",
       " ('psychological',),\n",
       " ('wounds',),\n",
       " ('painfully',),\n",
       " ('slow',),\n",
       " ('heal',),\n",
       " ('thank',),\n",
       " ('goodness',),\n",
       " ('always',),\n",
       " ('fishing',),\n",
       " ('anything',),\n",
       " ('catches',),\n",
       " ('footballs',),\n",
       " ('fish',),\n",
       " ('lake',),\n",
       " ('family',),\n",
       " ('times',),\n",
       " ('since',),\n",
       " ('never',),\n",
       " ('took',),\n",
       " ('bass',),\n",
       " ('within',),\n",
       " ('five',),\n",
       " ('thought',),\n",
       " ('skill',),\n",
       " ('angling',),\n",
       " ('answers',),\n",
       " ('landed',),\n",
       " ('secret',),\n",
       " ('patience',),\n",
       " ('smiling',),\n",
       " ('key',),\n",
       " ('ql',),\n",
       " ('begin',),\n",
       " ('influential',),\n",
       " ('commercial',),\n",
       " ('real',),\n",
       " ('predicted',),\n",
       " ('kick',),\n",
       " ('massive',),\n",
       " ('construction',),\n",
       " ('spurt',),\n",
       " ('equivalent',),\n",
       " ('basin',),\n",
       " ('growth',),\n",
       " ('spread',),\n",
       " ('throughout',),\n",
       " ('region',),\n",
       " ('jumps',),\n",
       " ('expected',),\n",
       " ('san',),\n",
       " ('fernando',),\n",
       " ('clarita',),\n",
       " ('valleys',),\n",
       " ('ontario',),\n",
       " ('concluded',),\n",
       " ('seeley',),\n",
       " ('co',),\n",
       " ('thursday',),\n",
       " ('already',),\n",
       " ('surrounding',),\n",
       " ('recession',),\n",
       " ('killed',),\n",
       " ('industrial',),\n",
       " ('middle',),\n",
       " ('decade',),\n",
       " ('upturn',),\n",
       " ('began',),\n",
       " ('turn',),\n",
       " ('boom',),\n",
       " ('beyond',),\n",
       " ('sprawling',),\n",
       " ('million',),\n",
       " ('square',),\n",
       " ('foot',),\n",
       " ('campus',),\n",
       " ('style',),\n",
       " ('office',),\n",
       " ('complex',),\n",
       " ('nine',),\n",
       " ('buildings',),\n",
       " ('planned',),\n",
       " ('acre',),\n",
       " ('site',),\n",
       " ('warner',),\n",
       " ('center',),\n",
       " ('suburban',),\n",
       " ('woodland',),\n",
       " ('hills',),\n",
       " ('prudential',),\n",
       " ('health',),\n",
       " ('care',),\n",
       " ('western',),\n",
       " ('regional',),\n",
       " ('headquarters',),\n",
       " ('lennar',),\n",
       " ('partners',),\n",
       " ('unit',),\n",
       " ('lnr',),\n",
       " ('property',),\n",
       " ('corp',),\n",
       " ('recently',),\n",
       " ('bought',),\n",
       " ('northeast',),\n",
       " ('corner',),\n",
       " ('canoga',),\n",
       " ('avenue',),\n",
       " ('burbank',),\n",
       " ('boulevard',),\n",
       " ('willing',),\n",
       " ('development',),\n",
       " ('ambitious',),\n",
       " ('project',),\n",
       " ('proposed',),\n",
       " ('area',),\n",
       " ('past',),\n",
       " ('plans',),\n",
       " ('call',),\n",
       " ('torn',),\n",
       " ('net',),\n",
       " ('amount',),\n",
       " ('expansion',),\n",
       " ('feet',),\n",
       " ('20th',),\n",
       " ('century',),\n",
       " ('industries',),\n",
       " ('largest',),\n",
       " ('employers',),\n",
       " ('valley',),\n",
       " ('broke',),\n",
       " ('ground',),\n",
       " ('tower',),\n",
       " ('next',),\n",
       " ('building',),\n",
       " ('completed',),\n",
       " ('december',),\n",
       " ('newhall',),\n",
       " ('ranch',),\n",
       " ('unanimous',),\n",
       " ('approval',),\n",
       " ('board',),\n",
       " ('supervisors',),\n",
       " ('involve',),\n",
       " ('homes',),\n",
       " ('includes',),\n",
       " ('space',),\n",
       " ('fact',),\n",
       " ('job',),\n",
       " ('occur',),\n",
       " ('residential',),\n",
       " ('rest',),\n",
       " ('come',),\n",
       " ('ventura',),\n",
       " ('counties',),\n",
       " ('inland',),\n",
       " ('empire',),\n",
       " ('thirds',),\n",
       " ('areas',),\n",
       " ('southeast',),\n",
       " ('yet',),\n",
       " ('built',),\n",
       " ('dennis',),\n",
       " ('macheski',),\n",
       " ('economist',),\n",
       " ('la',),\n",
       " ('riverside',),\n",
       " ('bernardino',),\n",
       " ('swell',),\n",
       " ('employment',),\n",
       " ('grow',),\n",
       " ('jobs',),\n",
       " ('accommodate',),\n",
       " ('build',),\n",
       " ('equal',),\n",
       " ('size',),\n",
       " ('retail',),\n",
       " ('housing',),\n",
       " ('units',),\n",
       " ('infrastructure',),\n",
       " ('require',),\n",
       " ('transportation',),\n",
       " ('facilities',),\n",
       " ('ted',),\n",
       " ('chief',),\n",
       " ('california',),\n",
       " ('department',),\n",
       " ('finance',),\n",
       " ('officials',),\n",
       " ('seemingly',),\n",
       " ('overwhelming',),\n",
       " ('planners',),\n",
       " ('developers',),\n",
       " ('concentrate',),\n",
       " ('refurbishing',),\n",
       " ('older',),\n",
       " ('rather',),\n",
       " ('carving',),\n",
       " ('projects',),\n",
       " ('undeveloped',),\n",
       " ('properties',),\n",
       " ('certain',),\n",
       " ('either',),\n",
       " ('changing',),\n",
       " ('uses',),\n",
       " ('higher',),\n",
       " ('michael',),\n",
       " ('ross',),\n",
       " ('vice',),\n",
       " ('president',),\n",
       " ('managing',),\n",
       " ('director',),\n",
       " ('division',),\n",
       " ('major',),\n",
       " ('investors',),\n",
       " ('look',),\n",
       " ('old',),\n",
       " ('aerospace',),\n",
       " ('plants',),\n",
       " ('starting',),\n",
       " ('purposes',),\n",
       " ('forecast',),\n",
       " ('might',),\n",
       " ('conservative',),\n",
       " ('considering',),\n",
       " ('annual',),\n",
       " ('predicting',),\n",
       " ('faster',),\n",
       " ('service',),\n",
       " ('industry',),\n",
       " ('lead',),\n",
       " ('gains',),\n",
       " ('entertainment',),\n",
       " ('rapid',),\n",
       " ('annually',),\n",
       " ('reports',),\n",
       " ('less',),\n",
       " ('city',),\n",
       " ('westside',),\n",
       " ('share',),\n",
       " ('tri',),\n",
       " ('cities',),\n",
       " ('glendale',),\n",
       " ('expand',),\n",
       " ('meanwhile',),\n",
       " ('manufacturing',),\n",
       " ('continue',),\n",
       " ('decline',),\n",
       " ('trend',),\n",
       " ('dates',),\n",
       " ('1950s',),\n",
       " ('local',),\n",
       " ('started',),\n",
       " ('retraction',),\n",
       " ('output',),\n",
       " ('however',),\n",
       " ('worker',),\n",
       " ('productivity',),\n",
       " ('tempe',),\n",
       " ('ariz',),\n",
       " ('tortured',),\n",
       " ('beholden',),\n",
       " ('blasted',),\n",
       " ('computers',),\n",
       " ('championship',),\n",
       " ('series',),\n",
       " ('tennessee',),\n",
       " ('florida',),\n",
       " ('monday',),\n",
       " ('night',),\n",
       " ('fiesta',),\n",
       " ('games',),\n",
       " ('weekend',),\n",
       " ('simply',),\n",
       " ('warm',),\n",
       " ('acts',),\n",
       " ('except',),\n",
       " ('dissent',),\n",
       " ('sugar',),\n",
       " ('matchup',),\n",
       " ('dares',),\n",
       " ('defy',),\n",
       " ('computer',),\n",
       " ('age',),\n",
       " ('undo',),\n",
       " ('bcs',),\n",
       " ('laid',),\n",
       " ('crown',),\n",
       " ('undisputed',),\n",
       " ('champion',),\n",
       " ('ohio',),\n",
       " ('rolls',),\n",
       " ('texas',),\n",
       " ('aampm',),\n",
       " ('squeaks',),\n",
       " ('ugly',),\n",
       " ('associated',),\n",
       " ('press',),\n",
       " ('poll',),\n",
       " ('presumably',),\n",
       " ('buckeyes',),\n",
       " ('coaches',),\n",
       " ('promised',),\n",
       " ('winner',),\n",
       " ('granted',),\n",
       " ('remote',),\n",
       " ('possibility',),\n",
       " ('coach',),\n",
       " ('john',),\n",
       " ('cooper',),\n",
       " ('likes',),\n",
       " ('idea',),\n",
       " ('floating',),\n",
       " ('ranked',),\n",
       " ('country',),\n",
       " ('finish',),\n",
       " ('maybe',),\n",
       " ('depends',),\n",
       " ('happens',),\n",
       " ('long',),\n",
       " ('shot',),\n",
       " ('exceptional',),\n",
       " ('win',),\n",
       " ('sloppy',),\n",
       " ('looked',),\n",
       " ('aggies',),\n",
       " ('ruined',),\n",
       " ('kansas',),\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index['content_index'].terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['content_index', 'title_index', 'content_doc_length', 'title_doc_length', 'average_content_length', 'average_title_length', 'content_idf', 'title_idf', 'content_sum_tf', 'title_sum_tf', 'content_sum_length', 'title_sum_length', 'content_collection_probability', 'title_collection_probability'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the queries from the file\n",
    "\n",
    "See the assignment description for the format of the query file [here](https://github.com/kbalog/uis-dat640-fall2019/tree/master/assignments/assignment-2a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(QUERY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25():\n",
    "    \n",
    "    def __init__(self, index, k1 = 1.2, b = 0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        \n",
    "        self.content_index = index['content_index']\n",
    "        self.content_idf = index['content_idf']\n",
    "        self.content_doc_length = index['content_doc_length']\n",
    "        self.average_content_length = index['average_content_length']\n",
    "\n",
    "    \n",
    "    def rank_query(self, query):\n",
    "        \n",
    "        # tokenize with nltk tokenizer because it works well\n",
    "        query = ' '.join([word for word in word_tokenize(query)])\n",
    "        \n",
    "        ranking = Counter()\n",
    "        \n",
    "        # tokenize a second time with hashedindex tokenizer to have tuple tokens\n",
    "        for token in textparser.word_tokenize(query, stopwords=stopwords, ngrams=N_GRAMS):\n",
    "            if token in self.content_index:\n",
    "                \n",
    "                documents = self.content_index.get_documents(token)\n",
    "                #print('\"{}\" appears in {} documents'.format(token, len(documents)))   \n",
    "                \n",
    "                ranking += Counter(self.rank_term(token, documents))\n",
    "            else:\n",
    "                print(token, 'is not in the index')   \n",
    "        \n",
    "        return ranking.most_common(100)\n",
    "        \n",
    "    def rank_term(self, term, documents):\n",
    "        # here we use a dictionnary because adding counters is a slow operation compared to dictionnary access\n",
    "        document_scores = {}\n",
    "        \n",
    "        for doc in documents:\n",
    "            tf = self.content_index.get_term_frequency(term, doc) # not normalized !\n",
    "            smoothing = 1 - self.b + self.b * (self.content_doc_length[doc]/self.average_content_length)         \n",
    "            score = ( ((1+self.k1)*tf) / (tf+self.k1*smoothing) ) * self.content_idf[term]\n",
    "            document_scores[doc] = score\n",
    "\n",
    "        return document_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM():\n",
    "    \n",
    "    def __init__(self, index, smoothing='jelinek', lambda_param=0.1, mu_param=1000):\n",
    "        \n",
    "        if smoothing == 'jelinek':\n",
    "            self.lambda_param = lambda_param\n",
    "        elif smoothing == 'dirichlet':\n",
    "            self.mu_param = mu_param\n",
    "        else:\n",
    "            raise ValueError('smoothing should in [jelinek, dirichlet]')\n",
    "            \n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "        self.content_index = index['content_index']\n",
    "        self.content_doc_length = index['content_doc_length']\n",
    "        self.content_sum_tf = index['content_sum_tf']\n",
    "        self.content_sum_length = index['content_sum_length']\n",
    "        self.content_collection_probability = index['content_collection_probability']\n",
    "        \n",
    "    \n",
    "    def rank_query(self, query):\n",
    "        query = ' '.join([word for word in word_tokenize(query)])\n",
    "        \n",
    "        # we can't use counter objects to track scores because it does not support negative addition\n",
    "        ranking = {}\n",
    "        \n",
    "        for token in textparser.word_tokenize(query, stopwords=stopwords, ngrams=N_GRAMS):\n",
    "            if token in self.content_index:\n",
    "                documents = self.content_index.get_documents(token)\n",
    "                #print('\"{}\" appears in {} documents'.format(token, len(documents)))\n",
    "                \n",
    "                if self.smoothing == 'jelinek':\n",
    "                    ranking = self.merge_rankings(ranking, self.rank_term_jelinek(token, documents))\n",
    "                elif self.smoothing == 'dirichlet':\n",
    "                    ranking = self.merge_rankings(ranking, self.rank_term_dirichlet(token, documents))\n",
    "                \n",
    "            else:\n",
    "                print(token, 'is not in the index')\n",
    "                \n",
    "        return sorted(ranking.items(), key=lambda x: x[1])[:100]\n",
    "        \n",
    "    def rank_term_jelinek(self, term, documents):\n",
    "        document_scores = {}\n",
    "        \n",
    "        for doc in documents:\n",
    "            ptd = self.content_index.get_term_frequency(term, doc, normalized=True)\n",
    "            ptc = self.content_collection_probability[term]\n",
    "            score = ((1 - self.lambda_param) * ptd) + (self.lambda_param * ptc)\n",
    "            document_scores[doc] = math.log(score)\n",
    "            \n",
    "        return document_scores\n",
    "    \n",
    "    def rank_term_dirichlet(self, term, documents):\n",
    "        document_scores = {}\n",
    "        \n",
    "        for doc in documents:\n",
    "            tf = self.content_index.get_term_frequency(term, doc, normalized=True)\n",
    "            ptc = self.content_collection_probability[term]\n",
    "            score = (tf + self.mu_param*ptc) / (self.content_doc_length[doc] + self.mu_param)\n",
    "            document_scores[doc] = math.log(score)\n",
    "            \n",
    "        return document_scores\n",
    "\n",
    "    def merge_rankings(self, base_ranking, to_add):\n",
    "        for doc, score in to_add.items():\n",
    "            base_ranking[doc] = base_ranking.get(doc, 0) + score\n",
    "        return base_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Generate a ranking for each query and output the results to `OUTPUT_FILE`\n",
    "\n",
    "See the assignment description for the format of the output file [here](https://github.com/kbalog/uis-dat640-fall2019/tree/master/assignments/assignment-2a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking documents for [303] 'Hubble Telescope Achievements'\n",
      "Ranking documents for [307] 'New Hydroelectric Projects'\n",
      "Ranking documents for [310] 'Radio Waves and Brain Cancer'\n",
      "Ranking documents for [314] 'Marine Vegetation'\n",
      "Ranking documents for [322] 'International Art Crime'\n",
      "Ranking documents for [325] 'Cult Lifestyles'\n",
      "Ranking documents for [330] 'Iran-Iraq Cooperation'\n",
      "Ranking documents for [336] 'Black Bear Attacks'\n",
      "Ranking documents for [341] 'Airport Security'\n",
      "Ranking documents for [344] 'Abuses of E-Mail'\n",
      "Ranking documents for [345] 'Overseas Tobacco Sales'\n",
      "Ranking documents for [347] 'Wildlife Extinction'\n",
      "Ranking documents for [353] 'Antarctica exploration'\n",
      "Ranking documents for [354] 'journalist risks'\n",
      "Ranking documents for [362] 'human smuggling'\n",
      "Ranking documents for [363] 'transportation tunnel disasters'\n",
      "Ranking documents for [367] 'piracy'\n",
      "Ranking documents for [372] 'Native American casino'\n",
      "Ranking documents for [374] 'Nobel prize winners'\n",
      "Ranking documents for [375] 'hydrogen energy'\n",
      "Ranking documents for [378] 'euro opposition'\n",
      "Ranking documents for [383] 'mental illness drugs'\n",
      "Ranking documents for [389] 'illegal technology transfer'\n",
      "Ranking documents for [393] 'mercy killing'\n",
      "Ranking documents for [394] 'home schooling'\n",
      "Ranking documents for [397] 'automobile recalls'\n",
      "Ranking documents for [399] 'oceanographic vessels'\n",
      "Ranking documents for [401] 'foreign minorities, Germany'\n",
      "Ranking documents for [404] 'Ireland, peace talks'\n",
      "Ranking documents for [408] 'tropical storms'\n",
      "Ranking documents for [409] 'legal, Pan Am, 103'\n",
      "Ranking documents for [416] 'Three Gorges Project'\n",
      "Ranking documents for [419] 'recycle, automobile tires'\n",
      "Ranking documents for [426] 'law enforcement, dogs'\n",
      "Ranking documents for [427] 'UV damage, eyes'\n",
      "Ranking documents for [433] 'Greek, philosophy, stoicism'\n",
      "Ranking documents for [435] 'curbing population growth'\n",
      "Ranking documents for [436] 'railway accidents'\n",
      "Ranking documents for [439] 'inventions, scientific discoveries'\n",
      "Ranking documents for [443] 'U.S., investment, Africa'\n",
      "Ranking documents for [448] 'ship losses'\n",
      "Ranking documents for [622] 'price fixing'\n",
      "Ranking documents for [625] 'arrests bombing WTC'\n",
      "Ranking documents for [638] 'wrongful convictions'\n",
      "Ranking documents for [639] 'consumer on-line shopping'\n",
      "Ranking documents for [648] 'family leave law'\n",
      "Ranking documents for [650] 'tax evasion indicted'\n",
      "Ranking documents for [651] 'U.S. ethnic population'\n",
      "Ranking documents for [658] 'teenage pregnancy'\n",
      "Ranking documents for [689] 'family-planning aid'\n"
     ]
    }
   ],
   "source": [
    "bm25 = BM25(index, k1=1.2, b=0.1)\n",
    "\n",
    "with open(BM25_OUTPUT_FILE, 'w') as f:\n",
    "    f.write('QueryId,DocumentId\\n')\n",
    "    \n",
    "    for q_id, query in queries.items():\n",
    "        \n",
    "        print(\"Ranking documents for [%s] '%s'\" % (q_id, query))\n",
    "        ranking = bm25.rank_query(query)\n",
    "        f.writelines(['{},{}\\n'.format(q_id, document[0], '\\n') for document in ranking])\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking documents for [303] 'Hubble Telescope Achievements'\n",
      "Ranking documents for [307] 'New Hydroelectric Projects'\n",
      "Ranking documents for [310] 'Radio Waves and Brain Cancer'\n",
      "Ranking documents for [314] 'Marine Vegetation'\n",
      "Ranking documents for [322] 'International Art Crime'\n",
      "Ranking documents for [325] 'Cult Lifestyles'\n",
      "Ranking documents for [330] 'Iran-Iraq Cooperation'\n",
      "Ranking documents for [336] 'Black Bear Attacks'\n",
      "Ranking documents for [341] 'Airport Security'\n",
      "Ranking documents for [344] 'Abuses of E-Mail'\n",
      "Ranking documents for [345] 'Overseas Tobacco Sales'\n",
      "Ranking documents for [347] 'Wildlife Extinction'\n",
      "Ranking documents for [353] 'Antarctica exploration'\n",
      "Ranking documents for [354] 'journalist risks'\n",
      "Ranking documents for [362] 'human smuggling'\n",
      "Ranking documents for [363] 'transportation tunnel disasters'\n",
      "Ranking documents for [367] 'piracy'\n",
      "Ranking documents for [372] 'Native American casino'\n",
      "Ranking documents for [374] 'Nobel prize winners'\n",
      "Ranking documents for [375] 'hydrogen energy'\n",
      "Ranking documents for [378] 'euro opposition'\n",
      "Ranking documents for [383] 'mental illness drugs'\n",
      "Ranking documents for [389] 'illegal technology transfer'\n",
      "Ranking documents for [393] 'mercy killing'\n",
      "Ranking documents for [394] 'home schooling'\n",
      "Ranking documents for [397] 'automobile recalls'\n",
      "Ranking documents for [399] 'oceanographic vessels'\n",
      "Ranking documents for [401] 'foreign minorities, Germany'\n",
      "Ranking documents for [404] 'Ireland, peace talks'\n",
      "Ranking documents for [408] 'tropical storms'\n",
      "Ranking documents for [409] 'legal, Pan Am, 103'\n",
      "Ranking documents for [416] 'Three Gorges Project'\n",
      "Ranking documents for [419] 'recycle, automobile tires'\n",
      "Ranking documents for [426] 'law enforcement, dogs'\n",
      "Ranking documents for [427] 'UV damage, eyes'\n",
      "Ranking documents for [433] 'Greek, philosophy, stoicism'\n",
      "Ranking documents for [435] 'curbing population growth'\n",
      "Ranking documents for [436] 'railway accidents'\n",
      "Ranking documents for [439] 'inventions, scientific discoveries'\n",
      "Ranking documents for [443] 'U.S., investment, Africa'\n",
      "Ranking documents for [448] 'ship losses'\n",
      "Ranking documents for [622] 'price fixing'\n",
      "Ranking documents for [625] 'arrests bombing WTC'\n",
      "Ranking documents for [638] 'wrongful convictions'\n",
      "Ranking documents for [639] 'consumer on-line shopping'\n",
      "Ranking documents for [648] 'family leave law'\n",
      "Ranking documents for [650] 'tax evasion indicted'\n",
      "Ranking documents for [651] 'U.S. ethnic population'\n",
      "Ranking documents for [658] 'teenage pregnancy'\n",
      "Ranking documents for [689] 'family-planning aid'\n"
     ]
    }
   ],
   "source": [
    "lm = LM(index, smoothing='jelinek', lambda_param=1.0)\n",
    "\n",
    "with open(LM_OUTPUT_FILE, 'w') as f:\n",
    "    f.write('QueryId,DocumentId\\n')\n",
    "    \n",
    "    for q_id, query in queries.items():\n",
    "        \n",
    "        print(\"Ranking documents for [%s] '%s'\" % (q_id, query))\n",
    "        ranking = lm.rank_query(query)\n",
    "        f.writelines(['{},{}\\n'.format(q_id, document[0], '\\n') for document in ranking])\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25_parameters_gridsearch(k_start=1.0, k_stop=2.1, k_step=0.1, b_start=0.0, b_stop=1.1, b_step=0.1):\n",
    "    k_range = np.arange(k_start, k_stop, k_step)\n",
    "    b_range = np.arange(b_start, b_stop, b_step)\n",
    "    grid = itertools.product(k_range, b_range)\n",
    "    \n",
    "    for k, b in grid:\n",
    "        print(k, b)\n",
    "        model = BM25(index, k1=k, b=b)\n",
    "        with open('data/gridsearch_bm25_k{:.3}_b{:.3}.txt'.format(k, b), 'w') as f:\n",
    "            f.write('QueryId,DocumentId\\n')\n",
    "\n",
    "            for q_id, query in queries.items():\n",
    "                ranking = model.rank_query(query)\n",
    "                f.writelines(['{},{}\\n'.format(q_id, document[0], '\\n') for document in ranking])\n",
    "                f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5ded21c9d42e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBM25_parameters_gridsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-d7b5d9fd92b9>\u001b[0m in \u001b[0;36mBM25_parameters_gridsearch\u001b[0;34m(k_start, k_stop, k_step, b_start, b_stop, b_step)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mq_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{},{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranking\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-a8c140abf58f>\u001b[0m in \u001b[0;36mrank_query\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is not in the index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrank_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36mmost_common\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_itemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_heapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_itemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/heapq.py\u001b[0m in \u001b[0;36mnlargest\u001b[0;34m(n, iterable, key)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0m_heapreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheapreplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtop\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0m_heapreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BM25_parameters_gridsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LM_parameters_gridsearch(lambda_start = 0.0, lambda_stop = 1.1, lambda_step = 0.1, mu_start=1, mu_stop=10000, n_mu=5):\n",
    "    smoothings = ['jelinek', 'dirichlet']\n",
    "    lambda_range = np.arange(lambda_start, lambda_stop, lambda_step)\n",
    "    mu_range = np.linspace(mu_start, mu_stop, n_mu)\n",
    "    grid = [[smoothings[0], param] for param in lambda_range] + [[smoothings[1], param] for param in mu_range]\n",
    "    \n",
    "    for params in grid:\n",
    "        print(params[0], params[1])\n",
    "        model = LM(index, smoothing=params[0], lambda_param=params[1], mu_param=params[1])\n",
    "        with open('data/gridsearch_lm_s{:.3}_param{:.3}.txt'.format(params[0], params[1]), 'w') as f:\n",
    "            f.write('QueryId,DocumentId\\n')\n",
    "\n",
    "            for q_id, query in queries.items():\n",
    "                ranking = model.rank_query(query)\n",
    "                f.writelines(['{},{}\\n'.format(q_id, document[0], '\\n') for document in ranking])\n",
    "                f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jelinek 0.0\n",
      "jelinek 0.1\n",
      "jelinek 0.2\n",
      "jelinek 0.30000000000000004\n",
      "jelinek 0.4\n",
      "jelinek 0.5\n",
      "jelinek 0.6000000000000001\n",
      "jelinek 0.7000000000000001\n",
      "jelinek 0.8\n",
      "jelinek 0.9\n",
      "jelinek 1.0\n",
      "dirichlet 1.0\n",
      "dirichlet 2500.75\n",
      "dirichlet 5000.5\n",
      "dirichlet 7500.25\n",
      "dirichlet 10000.0\n"
     ]
    }
   ],
   "source": [
    "LM_parameters_gridsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Report on the evaluation results (using the [Evaluation notebook](1_Evaluation.ipynb)) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the parameter settings used for the two methods:\n",
    "\n",
    "The models parameters were decided using a grid search over the reasonable values, ie $k1 \\in [1, 2]$, $b \\in [0, 1]$ for BM25 and $smoothing \\in [jelinek, dirichlet]$, $ \\lambda \\in [1, 2]$, $ \\mu \\in [10, 10000]$ for LM. Then, the values around the parameters yielding the best results were manually tested to check if some fine-tuning was possible. The parameters achieveing the highest mean accuracy precision were then tested on Kaggle.\n",
    "\n",
    "Write the name of the corresponding output file in the table. These files should be pushed to your repository.\n",
    "\n",
    "\n",
    "| **Method** | **Output file** | **Parameters** | **P@10** | **MAP** | **MRR** |\n",
    "| -- | -- | -- | -- | -- | -- |\n",
    "| BM25 | `data/bm25_singlefield.txt` | k1=1.2 b=0.1 | 0.2089 | 0.0793 | 0.4308 |\n",
    "| LM | `data/lm_singlefield.txt` | smoothing=jelinek, lambda=1.0 | 0.0778 | 0.0206 | 0.1399 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
