{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Assignment 3, Evalution\n",
    "\n",
    "This notebook can be used for evaluating an entity ranking against the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "BASE_RANKING_FILE = \"data/ranking_baseline.csv\"\n",
    "MLM_RANKING_FILE = \"data/ranking_model1.csv\"\n",
    "SDM_ELR_RANKING_FILE = \"data/ranking_model2.csv\"\n",
    "FSDM_ELR_RANKING_FILE = \"data/ranking_model3.csv\"\n",
    "QRELS_FILE = \"data/qrels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading ranking files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rankings(ranking_file):\n",
    "    rankings = {}\n",
    "    with open(ranking_file, \"r\") as fin:\n",
    "        header = fin.readline().strip()\n",
    "        if header != \"QueryId,EntityId\":\n",
    "            raise Exception(\"Incorrect file format!\")\n",
    "        for line in fin.readlines():\n",
    "            qid, _ = line.strip().split(\",\\\"\")\n",
    "            docid = _[:-1]  # remove trailing \"\n",
    "            if qid not in rankings:\n",
    "                rankings[qid] = []\n",
    "            rankings[qid].append(docid)\n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading relevance judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qrels(qrels_file):\n",
    "    qrels = {}\n",
    "    with open(qrels_file, \"r\") as fin:\n",
    "        header = fin.readline().strip()\n",
    "        if header != \"QueryId,EntityId,Relevance\":\n",
    "            raise Exception(\"Incorrect file format!\")\n",
    "        for line in fin.readlines():\n",
    "            qid, _ = line.strip().split(\",\\\"\")\n",
    "            docid, rel = _.split(\"\\\",\")\n",
    "            if qid not in qrels:\n",
    "                qrels[qid] = {}\n",
    "            qrels[qid][docid] = int(rel)\n",
    "    return qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing NDCG scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(rel, p):\n",
    "    dcg = rel[0]\n",
    "    for i in range(1, min(p, len(rel))): \n",
    "        dcg += rel[i] / math.log(i + 1, 2)  # rank position is indexed from 1..\n",
    "    return dcg\n",
    "\n",
    "def compute_ndcg(rankings, qrels, k=100):\n",
    "    sum_ndcg = 0\n",
    "    for qid, ranking in sorted(rankings.items()):\n",
    "        gt = qrels[qid]    \n",
    "        gains = []  # holds corresponding relevance levels for the ranked docs\n",
    "        for doc_id in ranking[:k]: \n",
    "            gain = gt.get(doc_id, 0)\n",
    "            gains.append(gain)\n",
    "\n",
    "        # relevance levels of the idealized ranking\n",
    "        gain_ideal = sorted([v for _, v in gt.items()], reverse=True)\n",
    "\n",
    "        ndcg = dcg(gains, k) / dcg(gain_ideal, k)\n",
    "        sum_ndcg += ndcg\n",
    "\n",
    "    return sum_ndcg / len(rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ranking_baseline.csv\n",
      "\tNDCG@10: 0.3791\n",
      "\tNDCG@100: 0.4421\n",
      "\n",
      "data/ranking_model1.csv\n",
      "\tNDCG@10: 0.3410\n",
      "\tNDCG@100: 0.4114\n",
      "\n",
      "data/ranking_model2.csv\n",
      "\tNDCG@10: 0.3190\n",
      "\tNDCG@100: 0.3987\n",
      "\n",
      "data/ranking_model3.csv\n",
      "\tNDCG@10: 0.3424\n",
      "\tNDCG@100: 0.4132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qrels = load_qrels(QRELS_FILE)\n",
    "\n",
    "for ranking in [BASE_RANKING_FILE, MLM_RANKING_FILE, SDM_ELR_RANKING_FILE, FSDM_ELR_RANKING_FILE]:\n",
    "    rankings = load_rankings(ranking)\n",
    "    print(f'{ranking}')\n",
    "    print(f'\\tNDCG@10: {compute_ndcg(rankings, qrels, k=10):.4f}')\n",
    "    print(f'\\tNDCG@100: {compute_ndcg(rankings, qrels, k=100):.4f}', end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
