{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3, Model 2: SDM+ELR\n",
    "\n",
    "In this notebook you will implement SDM+ELR re-ranking of the first-pass ranking retrieved from your index.\n",
    "\n",
    "Your implementation of the sequential dependence model (SDM) extended with the Entity-linking incorporated retrieval (ELR) will be a single-field variant of SDM, so you need to use the \"catch-all\" text field for scoring. \n",
    "\n",
    "Your implementation should use the following weights: 0.80 for unigram matches, 0.05 for ordered bigram matches, 0.05 for unordered bigram matches, and 0.1 for entity matches. \n",
    "\n",
    "Dirichlet smoothing should be applied with the smoothing parameter set to 2000. \n",
    "\n",
    "The entity annotations for queries are provided as part of the input.\n",
    "\n",
    "Be sure to use both markdown cells with section headings and explanations, as well as writing readable code, to make it clear what your intention is each step of the way through the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'HoG',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'Zle62SN2R0-CJz5T3d07Og',\n",
       " 'version': {'number': '7.4.2',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'deb',\n",
       "  'build_hash': '2f90bbf7b93631e52bafb59b3b049cb44ec25e96',\n",
       "  'build_date': '2019-10-28T20:40:44.881551Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.2.0',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = 'collection_v2'\n",
    "weights = {\n",
    "    'terms' : 0.8,\n",
    "    'ordered_bigrams' : 0.05,\n",
    "    'unordered_bigrams' : 0.05,\n",
    "    'entities' : 0.1,\n",
    "}\n",
    "\n",
    "PENALTY = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File loading utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(path):\n",
    "    queries = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            query = line.split(maxsplit = 1)\n",
    "            queries[query[0]] = query[1].strip()\n",
    "    return queries\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to rename each entity when exporting the ranking because the expected format is not the same as the one from the indexed files.\n",
    "For example, `<http://dbpedia.org/resource/Feature_Selection>` translates to `<dbpedia:Feature_Selection>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(entity):\n",
    "    if entity.startswith('<http'):\n",
    "        basename = entity.split('/')[-1]\n",
    "        return f'<dbpedia:{basename}'\n",
    "    elif entity.startswith('<db'):\n",
    "        basename = entity.split(':')[-1]\n",
    "        return f'<http://dbpedia.org/resource/{basename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_file = 'data/ranking_model2.csv'\n",
    "queries = load_queries('data/queries.txt')\n",
    "entity_annotations = read_json('data/entity_annotations.json')\n",
    "collection_stats = read_json('data/collection_stats.json')\n",
    "\n",
    "# translation process\n",
    "for query_id, annotation in entity_annotations.items():\n",
    "    for entity in annotation:\n",
    "        entity_annotations[query_id][entity]['uri'] = translate(entity_annotations[query_id][entity]['uri'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(query):\n",
    "    response = es.indices.analyze(index=INDEX_NAME, body={'text': query, 'analyzer':'english_analyzer'})\n",
    "    analyzed_query = [term['token'] for term in response['tokens']]\n",
    "    return analyzed_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term field frequency retrieval\n",
    "\n",
    "Sometimes, the total term frequencies in a field for a specific term is not available directy in the termvectors because despite a great BM25 score, this specific term is not in the field of the document being scored. To retrieve this information, we search for this term in the index in order to find its `ttf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ttf(term, tv, field='catch_all'):\n",
    "    if term in tv['terms']:\n",
    "        return tv['terms'][term]['ttf']\n",
    "        \n",
    "    else:\n",
    "        query = { 'size' : 10, 'query': { 'match' : { f'{field}': f'{term}' } } }\n",
    "        docs = es.search(index=INDEX_NAME, body=query)['hits']['hits']\n",
    "\n",
    "        for doc in docs:\n",
    "            tv = es.termvectors(index=INDEX_NAME, id=doc['_id'], fields=field, term_statistics=True)['term_vectors']\n",
    "            if field in tv and term in tv[field]['terms']:\n",
    "                return tv[field]['terms'][term]['ttf']\n",
    "    \n",
    "    # if ttf could not be found, skip this term in the scoring\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get term sequence from termvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_sequence(termvector):\n",
    "    term_positions = {}\n",
    "    for term, info in termvector['terms'].items():\n",
    "        for token in info['tokens']:\n",
    "            term_positions[token['position']] = term\n",
    "\n",
    "    term_sequence = [None] * (max(term_positions.keys()) + 1)\n",
    "    for position, term in term_positions.items():\n",
    "        term_sequence[position] = term\n",
    "\n",
    "    return term_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDM + ELR ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sdm_elr(documents, query, query_entities):\n",
    "    scores = {}\n",
    "    query_terms = analyze(query)\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_id = doc['_id']\n",
    "        termvectors = es.termvectors(index=INDEX_NAME, id=doc_id, fields='catch_all, related_entity_uri', term_statistics=True, field_statistics=True)['term_vectors']\n",
    "        scores[doc_id] = SDM_ELR_score(termvectors, query_terms, query_entities)\n",
    "        \n",
    "    sorted_scores = sorted(scores.items(), key = lambda pair: pair[1], reverse=True)\n",
    "    return [doc[0] for doc in sorted_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SDM_ELR_score(termvectors, query_terms, query_entities):     \n",
    "    lambda_t = weights['terms'] /  len(query_terms)\n",
    "    lambda_e = weights['entities'] /  sum(x['score'] for x in query_entities.values())\n",
    "    if len(query_terms) > 1:\n",
    "        lambda_o = weights['ordered_bigrams'] /  (len(query_terms) - 1)\n",
    "        lambda_u = weights['unordered_bigrams'] /  (len(query_terms) - 1)\n",
    "    else:\n",
    "        lambda_o = 0 \n",
    "        lambda_u = 0\n",
    "        \n",
    "    if 'catch_all' in termvectors:\n",
    "        term_sequence = get_term_sequence(termvectors['catch_all'])\n",
    "    else:\n",
    "        term_sequence = []\n",
    "        \n",
    "    return sum([lambda_t*term_score(termvectors, query_terms),\n",
    "               lambda_o*bigram_score(term_sequence, query_terms, ordered=True),\n",
    "               lambda_u*bigram_score(term_sequence, query_terms, ordered=False),\n",
    "               lambda_e*entity_score(termvectors, query_entities)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram match score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_score(termvectors, query_terms, field='catch_all', mu_param=2000):\n",
    "    score = 0\n",
    "    termvectors = termvectors.get(field, {})\n",
    "\n",
    "    for term in query_terms:\n",
    "        if 'terms' in termvectors:\n",
    "            ftd = termvectors['terms'].get(term, {}).get('term_freq', 0)\n",
    "            doc_length = sum(term['term_freq'] for term in termvectors['terms'].values())  \n",
    "            sum_ftd = find_ttf(term, termvectors)\n",
    "            field_length = collection_stats[field]['sum_ttf']\n",
    "            ptc = sum_ftd / field_length\n",
    "            term_score = (ftd + mu_param * ptc) / (doc_length + mu_param)\n",
    "            score += math.log(term_score) if term_score > 0 else math.log(PENALTY)\n",
    "            \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram match scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ordered_bigram_matches(text, bigram):\n",
    "    \"\"\"Counts the number of bigram matches in term sequence.\"\"\"\n",
    "    count = 0\n",
    "    for term, next_term in zip(text, text[1:]):\n",
    "        if (term, next_term) == bigram:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_unordered_bigram_matches(text, bigram, w=4):\n",
    "    \"\"\"Counts the number of unordered bigram matches in text within a given window size.\"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] in bigram:\n",
    "            other_term = bigram[0] if text[i] == bigram[1] else bigram[1]\n",
    "            if other_term in text[i+1:i+w]:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_background_model(bigram, ordered, w=5):\n",
    "    slop = 0 if ordered else w\n",
    "    \n",
    "    query = {\n",
    "        \"track_total_hits\": True,\n",
    "        \"size\":0,\n",
    "        \"query\": { \n",
    "            \"match_phrase\": { \n",
    "                \"catch_all\": { \n",
    "                    \"query\": f\"{bigram[0]} {bigram[1]}\", \n",
    "                    \"slop\": slop\n",
    "                } \n",
    "            } \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # background model approximation\n",
    "    docs = es.search(index=INDEX_NAME, body=query)['hits']\n",
    "    n_bigrams_collection = docs['total']['value']\n",
    "\n",
    "    return n_bigrams_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_score(term_sequence, query_terms, ordered, mu_param=2000):\n",
    "    score = 0\n",
    "    doc_len = len(term_sequence)\n",
    "    \n",
    "    # define bigram counting method\n",
    "    bigram_count = count_ordered_bigram_matches if ordered else count_unordered_bigram_matches\n",
    "    \n",
    "    for bigram in zip(query_terms, query_terms[1:]):\n",
    "        count = bigram_count(term_sequence, bigram)\n",
    "        n_bigrams_background = bigram_background_model(bigram, ordered)\n",
    "        background_model = n_bigrams_background / collection_stats['catch_all']['sum_ttf']\n",
    "        raw_score = (count + mu_param * background_model) / (doc_len + mu_param)\n",
    "        raw_score = count / doc_len if doc_len != 0 else 0\n",
    "        score += math.log(raw_score) if raw_score != 0 else math.log(PENALTY)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_score(termvectors, query_entities, lambda_param=0.1):\n",
    "    score = 0\n",
    "    termvectors = termvectors.get('related_entity_uri', {})\n",
    "    \n",
    "    for entity_name, annotation in query_entities.items():\n",
    "        feature_function = 0\n",
    "        if annotation['uri'] in termvectors.get('terms', {}):\n",
    "            feature_function += (1-lambda_param)\n",
    "        feature_function += lambda_param * find_ttf(annotation['uri'], termvectors, field='related_entity_uri')\n",
    "        if feature_function == 0:\n",
    "            score += annotation['score'] * math.log(PENALTY)\n",
    "        else:\n",
    "            score += annotation['score'] * math.log(feature_function)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting rankings to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_ranking(ranking, path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write('QueryId,EntityId\\n')\n",
    "        for query_id, entity_list in ranking.items():\n",
    "            for entity in entity_list:\n",
    "                f.write(f'{query_id},\"{translate(entity)}\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking(rank_method):\n",
    "    ranking = {}\n",
    "    \n",
    "    for query_id, query in tqdm(queries.items()):\n",
    "        # retrieve first 100 hits using the default retrieval model\n",
    "        first_pass = es.search(index=INDEX_NAME, q=query, size=100)['hits']['hits']\n",
    "        query_entities = entity_annotations[query_id]\n",
    "        ranking[query_id] = rank_method(first_pass, query, query_entities)\n",
    "        \n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [14:37<00:00,  3.75s/it]\n"
     ]
    }
   ],
   "source": [
    "sdm_elr_ranking = compute_ranking(rank_sdm_elr)\n",
    "export_ranking(sdm_elr_ranking, ranking_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting rankings for the two query sets should be saved and pushed to GitHub as `data/ranking_model2.csv` and `data/ranking2_model2.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
