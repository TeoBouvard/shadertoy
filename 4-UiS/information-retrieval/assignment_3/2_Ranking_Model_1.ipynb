{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3, Model 1: MLM\n",
    "\n",
    "In this notebook you will implement MLM re-ranking of the first-pass ranking retrieved from your index. \n",
    "\n",
    "Your implementation of the mixture of language models (MLM) approach should work with two fields, `title` and `content`, with weights 0.2 and 0.8, respectively. \n",
    "\n",
    "Content should be the \"catch-all\" field. Use Dirichlet smoothing with the smoothing parameter set to 2000.\n",
    "\n",
    "Be sure to use both markdown cells with section headings and explanations, as well as writing readable code, to make it clear what your intention is each step of the way through the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'HoG',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'Zle62SN2R0-CJz5T3d07Og',\n",
       " 'version': {'number': '7.4.2',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'deb',\n",
       "  'build_hash': '2f90bbf7b93631e52bafb59b3b049cb44ec25e96',\n",
       "  'build_date': '2019-10-28T20:40:44.881551Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.2.0',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = 'collection_v2'\n",
    "fields = {\n",
    "    'names' : 0.2,\n",
    "    'catch_all' : 0.8   \n",
    "}\n",
    "\n",
    "PENALTY = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File loading utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(path):\n",
    "    queries = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            query = line.split(maxsplit = 1)\n",
    "            queries[query[0]] = query[1].strip()\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_RANKING_FILE = 'data/ranking2_baseline.csv'\n",
    "MLM_RANKING_FILE = 'data/ranking2_model1.csv'\n",
    "queries = load_queries('data/queries2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(query):\n",
    "    response = es.indices.analyze(index=INDEX_NAME, body={'text': query, 'analyzer':'english_analyzer'})\n",
    "    analyzed_query = [term['token'] for term in response['tokens']]\n",
    "    return analyzed_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term field frequency retrieval\n",
    "\n",
    "Sometimes, the total term frequencies in a field for a specific term is not available directy in the termvectors because despite a great BM25 score, this specific term is not in the field of the document being scored. To retrieve this information, we search for this term in the index in order to find its `ttf`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ttf(term, tv, field):\n",
    "    if term in tv['terms']:\n",
    "        return tv['terms'][term]['ttf']\n",
    "        \n",
    "    else:\n",
    "        query = { 'size' : 10, 'query': { 'match' : { f'{field}': f'{term}' } } }\n",
    "        docs = es.search(index=INDEX_NAME, body=query)['hits']['hits']\n",
    "\n",
    "        for doc in docs:\n",
    "            tv = es.termvectors(index=INDEX_NAME, id=doc['_id'], fields=field, term_statistics=True)['term_vectors']\n",
    "            if field in tv and term in tv[field]['terms']:\n",
    "                return tv[field]['terms'][term]['ttf']\n",
    "    \n",
    "    # if ttf could not be found, skip this term in the scoring\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline ranking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_baseline(documents, query):\n",
    "    scores = {}\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_id = doc['_id']\n",
    "        scores[doc_id] = doc['_score']\n",
    "        \n",
    "    sorted_scores = sorted(scores.items(), key = lambda pair: pair[1], reverse=True)\n",
    "    return [doc[0] for doc in sorted_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM Ranking and scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_mlm(documents, query):\n",
    "    scores = {}\n",
    "    query_terms = analyze(query)\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_id = doc['_id']\n",
    "        termvectors = es.termvectors(index=INDEX_NAME, id=doc_id, fields=list(fields.keys()), field_statistics=True, term_statistics=True)['term_vectors']\n",
    "        scores[doc_id] = MLM_score(termvectors, query_terms)\n",
    "\n",
    "    sorted_scores = sorted(scores.items(), key = lambda pair: pair[1], reverse=True)\n",
    "    return [doc[0] for doc in sorted_scores]\n",
    "\n",
    "\n",
    "def MLM_score(termvectors, query_terms):\n",
    "    mlm_score = 0\n",
    "    for field_name, field_weight in fields.items():\n",
    "        mlm_score += field_weight * LM_score(termvectors, field_name, query_terms)\n",
    "    return mlm_score\n",
    "\n",
    "\n",
    "def LM_score(termvectors, field, query_terms, mu_param=2000):\n",
    "    score = 0\n",
    "    termvectors = termvectors.get(field, {})\n",
    "    \n",
    "    for term in query_terms:\n",
    "        if 'terms' in termvectors:\n",
    "            ftd = termvectors['terms'].get(term, {}).get('term_freq', 0)\n",
    "            doc_length = sum(term['term_freq'] for term in termvectors['terms'].values())\n",
    "            sum_ftd = find_ttf(term, termvectors, field)\n",
    "            field_length = termvectors['field_statistics']['sum_ttf']\n",
    "            ptc = sum_ftd / field_length\n",
    "            term_score = (ftd + mu_param * ptc) / (doc_length + mu_param)\n",
    "            score += math.log(term_score) if term_score > 0 else math.log(PENALTY)\n",
    "            \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting rankings to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to rename each entity when exporting the ranking because the expected format is not the same as the one from the indexed files.\n",
    "For example, `<http://dbpedia.org/resource/Feature_Selection>` translates to `<dbpedia:Feature_Selection>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(entity):\n",
    "    basename = entity.split('/')[-1]\n",
    "    return f'\"<dbpedia:{basename}\"'\n",
    "\n",
    "def export_ranking(ranking, path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write('QueryId,EntityId\\n')\n",
    "        for query_id, entity_list in ranking.items():\n",
    "            for entity in entity_list:\n",
    "                f.write(f'{query_id},{rename(entity)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_ranking(rank_method):\n",
    "    ranking = {}\n",
    "    \n",
    "    for query_id, query in tqdm(queries.items()):\n",
    "        # retrieve first 100 hits using the default retrieval model\n",
    "        first_pass = es.search(index=INDEX_NAME, q=query, size=100)['hits']['hits']\n",
    "        \n",
    "        # rerank the first pass using custom ranking method\n",
    "        ranking[query_id] = rank_method(first_pass, query)\n",
    "        \n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233/233 [00:49<00:00,  4.68it/s]\n"
     ]
    }
   ],
   "source": [
    "baseline_ranking = compute_ranking(rank_baseline)\n",
    "export_ranking(baseline_ranking, BASELINE_RANKING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233/233 [13:10<00:00,  3.39s/it]\n"
     ]
    }
   ],
   "source": [
    "mlm_ranking = compute_ranking(rank_mlm)\n",
    "export_ranking(mlm_ranking, MLM_RANKING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting rankings for the two query sets should be saved and pushed to GitHub as `data/ranking_model1.csv` and `data/ranking2_model1.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
