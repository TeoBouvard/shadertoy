\documentclass[a4paper, 10pt, twoside]{article}
\usepackage[left=2cm, right=2cm, top=2cm, bottom=3cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}

\begin{document}

\title{Machine Learning - Theoretical exercise 6}
\author{T\'eo Bouvard}
\maketitle

\section*{Problem 1}

\newcommand{\ED}[1]{\mathbb{E}_\mathcal{D}\left[#1\right]}

To prove this equality in a readable way, we will denote $g \equiv g\left(x;\mathcal{D}\right)$ and $F \equiv F(x)$.

\begin{align*}
	\ED{(g-F)^2}
	 & = \ED{(g \underbrace{- \ED{g} + \ED{g}}_{= 0} - F)^2}                         \\
	 & = \ED{(g - \ED{g})^2 + 2 (g - \ED{g})(\ED{g} - F) + (\ED{g} - F)^2}           \\
	 & = \ED{(g - \ED{g})^2} + 2 \ED{(g - \ED{g})(\ED{g} - F)} + \ED{(\ED{g} - F)^2} \\
\end{align*}

Until now, we only injected $\ED{g}$ into the equation, expanded the identity, and used the linearity of expectation property of the expectation operator to break it down in three terms. We remark that the left term is equal to the variance. The right term can be simplifed because it is the expected value of a $\ED{g} - F$, which is a constant.

\begin{align*}
	\ED{(g-F)^2} & = \ED{(g - \ED{g})^2} + 2 \ED{(g - \ED{g})(\ED{g} - F)} + (\ED{g} - F)^2 \\
\end{align*}

We now see that the right term is equal to the bias squared. What do we do with the central term ? As we have remarked above, $\ED{g} - F$ is a constant so we can get it out of the expectation operator.

\begin{align*}
	\ED{(g-F)^2}
	 & = \ED{(g - \ED{g})^2} + 2 (\ED{g} - F) \ED{(g - \ED{g})} + (\ED{g} - F)^2                   \\
	 & = \ED{(g - \ED{g})^2} + 2 (\ED{g} - F) (\ED{g} - \ED{\ED{g}}) + (\ED{g} - F)^2              \\
	 & = \ED{(g - \ED{g})^2} + 2 (\ED{g} - F) \underbrace{(\ED{g} - \ED{g})}_{=0} + (\ED{g} - F)^2 \\
	 & = \underbrace{\ED{(g - \ED{g})^2}}_{variance} + \underbrace{(\ED{g} - F)^2}_{bias^2}        \\
\end{align*}

\section*{Problem 2}

\newcommand{\fracN}{\frac{1}{N}}
\newcommand{\fracM}{\frac{1}{N-1}}
\newcommand{\sumNi}{\sum_{i=1}^N}
\newcommand{\sumNj}{\sum_{j=1}^N}
\newcommand{\sumM}{\sum_{j=1, j \ne i}^N}

\begin{align*}
	\mu_{(\cdot)}
	 & = \fracN \sumNi \mu_{(i)}                                             \\
	 & = \fracN \sumNi \fracM \sumM x_j                                      \\
	 & = \fracN \fracM \sumNi \sumM x_j                                      \\
	 & = \fracN \fracM \sumNi \left[ \left( \sumNj x_j \right) - x_i \right] \\
	 & = \fracN \fracM \left[ N \sumNj x_j - \sumNi x_i \right]              \\
	 & = \fracN \fracM (N-1) \sumNi x_i                                      \\
	 & = \fracN \sumNi x_i                                                   \\
	 & = \hat{\mu}                                                           \\
\end{align*}

\section*{Problem 3}

\newcommand{\normsq}[1]{\Vert #1 \Vert^2}

For each clustering, we compute the sum of squared error $J_e$.

\begin{enumerate}
	\item We first compute the centroid of each cluster.

	      \begin{align*}
		      m_1 & =
		      \frac{1}{2} (x_1 + x_2) =
		      \frac{1}{2} \left(
		      \begin{bmatrix} 0 \\ 0 \end{bmatrix} +
		      \begin{bmatrix} 1 \\ 1 \end{bmatrix}
		      \right)                  =
		      \begin{bmatrix} 0.5 \\ 0.5 \end{bmatrix} \\
		      m_2 & =
		      \frac{1}{2} (x_3 + x_4) =
		      \frac{1}{2} \left(
		      \begin{bmatrix} 1 \\ 0 \end{bmatrix} +
		      \begin{bmatrix} 2 \\ 0.5 \end{bmatrix}
		      \right)                 =
		      \begin{bmatrix} 1.5 \\ 0.25 \end{bmatrix}
	      \end{align*}

	      And then the squared error for each point.

	      \begin{align*}
		      \normsq{x_1 - m_1} = (0-0.5)^2 + (0-0.5)^2 = 0.5       \\
		      \normsq{x_2 - m_1} = (1-0.5)^2 + (1-0.5)^2 = 0.5       \\
		      \normsq{x_3 - m_2} = (1-1.5)^2 + (0-0.25)^2 = 0.3125   \\
		      \normsq{x_4 - m_2} = (2-1.5)^2 + (0.5-0.25)^2 = 0.3125 \\
	      \end{align*}

	      And finally, the we compute $J_e$ as the sum of all errors.

	      \begin{align*}
		      J_e^{(1)} = 0.5 + 0.5 + 0.3125 + 0.3125 = 1.625
	      \end{align*}

	\item We repeat the same process for the second clustering. Centroids first,

	      \begin{align*}
		      m_1 & =
		      \frac{1}{2} (x_1 + x_4) =
		      \frac{1}{2} \left(
		      \begin{bmatrix} 0 \\ 0 \end{bmatrix} +
		      \begin{bmatrix} 2 \\ 0.5 \end{bmatrix}
		      \right)                  =
		      \begin{bmatrix} 1 \\ 0.25 \end{bmatrix} \\
		      m_2 & =
		      \frac{1}{2} (x_2 + x_3) =
		      \frac{1}{2} \left(
		      \begin{bmatrix} 1 \\ 1 \end{bmatrix} +
		      \begin{bmatrix} 1 \\ 0 \end{bmatrix}
		      \right)                 =
		      \begin{bmatrix} 1 \\ 0.5 \end{bmatrix}
	      \end{align*}

	      Then individual errors,

	      \begin{align*}
		      \normsq{x_1 - m_1} = (0-1)^2 + (0-0.25)^2 = 1.0625   \\
		      \normsq{x_4 - m_1} = (2-1)^2 + (0.5-0.25)^2 = 1.0625 \\
		      \normsq{x_2 - m_2} = (1-1)^2 + (1-0.5)^2 = 0.25      \\
		      \normsq{x_3 - m_2} = (1-1)^2 + (0-0.5)^2 = 0.25      \\
	      \end{align*}

	      And sum of errors

	      \begin{align*}
		      J_e^{(2)} = 1.0625 + 1.0625 + 0.25 + 0.25 = 2.625
	      \end{align*}

	\item We repeat the same process for the third clustering. Centroids first,

	      \begin{align*}
		      m_1 & =
		      \frac{1}{3} (x_1 + x_2 + x_3) =
		      \frac{1}{3} \left(
		      \begin{bmatrix} 0 \\ 0 \end{bmatrix} +
		      \begin{bmatrix} 1 \\ 1 \end{bmatrix} +
		      \begin{bmatrix} 1 \\ 0 \end{bmatrix}
		      \right)                  =
		      \begin{bmatrix} \frac{2}{3} \\ \frac{1}{3} \end{bmatrix} \\
		      m_2 & =
		      \frac{1}{1} (x_4) =
		      \begin{bmatrix} 2 \\ 0.5 \end{bmatrix}
	      \end{align*}

	      Then individual errors,

	      \begin{align*}
		      \normsq{x_1 - m_1} = (0-\frac{2}{3})^2 + (0-\frac{1}{3})^2 = \frac{5}{9} \\
		      \normsq{x_2 - m_1} = (1-\frac{2}{3})^2 + (1-\frac{1}{3})^2 = \frac{5}{9} \\
		      \normsq{x_3 - m_1} = (1-\frac{2}{3})^2 + (0-\frac{1}{3})^2 = \frac{2}{9} \\
		      \normsq{x_4 - m_2} = (2-2)^2 + (0.5-0.5)^2 = 0                           \\
	      \end{align*}

	      And sum of errors

	      \begin{align*}
		      J_e^{(3)} = \frac{5}{9} + \frac{5}{9} + \frac{5}{9} + 0 = \frac{5}{3}
	      \end{align*}
\end{enumerate}
Since $\frac{5}{3} < 1.625 < 2.625$, clustering 3. is the clustering minimizing $J_e$.

\section*{Problem 4}
\begin{enumerate}[a)]
	\item First of all, we assign each sample to its closest cluster, which leads to the following repartition.

	      \begin{align*}
		      C_1 & = \{ x_3, x_4, x_6, x_7\} \\
		      C_2 & = \{ x_1, x_2, x_5 \}
	      \end{align*}

	      We then follow the algorithm by taking the random samples in the given order.

	      \begin{itemize}
		      \item Sample chosen : $x_1$

	      \end{itemize}

\end{enumerate}

\end{document}
