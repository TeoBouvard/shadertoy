\documentclass[a4paper, 10pt, twoside]{article}
\usepackage[left=2cm, right=2cm, top=2cm, bottom=3cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}

\begin{document}

\title{Machine Learning - Theoretical exercise 6}
\author{T\'eo Bouvard}
\maketitle

\section*{Problem 1}

\newcommand{\ED}[1]{\mathbb{E}_\mathcal{D}\left[#1\right]}

To prove this equality in a readable way, we will denote $g \equiv g\left(x;\mathcal{D}\right)$ and $F \equiv F(x)$.

\begin{align*}
	\ED{(g-F)^2}
	 & = \ED{(g \underbrace{- \ED{g} + \ED{g}}_{= 0} - F)^2}                         \\
	 & = \ED{(g - \ED{g})^2 + 2 (g - \ED{g})(\ED{g} - F) + (\ED{g} - F)^2}           \\
	 & = \ED{(g - \ED{g})^2} + 2 \ED{(g - \ED{g})(\ED{g} - F)} + \ED{(\ED{g} - F)^2} \\
\end{align*}

Until now, we only injected $\ED{g}$ into the equation, expanded the identity, and used the linearity of expectation property of the expectation operator to break it down in three terms. We remark that the left term is equal to the variance. The right term can be simplified because it is the expected value of a $\ED{g} - F$, which is a constant.

\begin{align*}
	\ED{(g-F)^2} & = \ED{(g - \ED{g})^2} + 2 \ED{(g - \ED{g})(\ED{g} - F)} + (\ED{g} - F)^2 \\
\end{align*}

We now see that the right term is equal to the bias squared. What do we do with the central term ? As we have remarked above, $\ED{g} - F$ is a constant so we can get it out of the expectation operator.

\begin{align*}
	\ED{(g-F)^2}
	 & = \ED{(g - \ED{g})^2} + 2 (\ED{g} - F) \ED{(g - \ED{g})} + (\ED{g} - F)^2                   \\
	 & = \ED{(g - \ED{g})^2} + 2 (\ED{g} - F) (\ED{g} - \ED{\ED{g}}) + (\ED{g} - F)^2              \\
	 & = \ED{(g - \ED{g})^2} + 2 (\ED{g} - F) \underbrace{(\ED{g} - \ED{g})}_{=0} + (\ED{g} - F)^2 \\
	 & = \underbrace{\ED{(g - \ED{g})^2}}_{variance} + \underbrace{(\ED{g} - F)^2}_{bias^2}        \\
\end{align*}

\section*{Problem 2}

\newcommand{\fracN}{\frac{1}{N}}
\newcommand{\fracM}{\frac{1}{N-1}}
\newcommand{\sumNi}{\sum_{i=1}^N}
\newcommand{\sumNj}{\sum_{j=1}^N}
\newcommand{\sumM}{\sum_{j=1, j \ne i}^N}

\begin{align*}
	\mu_{(\cdot)}
	 & = \fracN \sumNi \mu_{(i)}                                             \\
	 & = \fracN \sumNi \fracM \sumM x_j                                      \\
	 & = \fracN \fracM \sumNi \sumM x_j                                      \\
	 & = \fracN \fracM \sumNi \left[ \left( \sumNj x_j \right) - x_i \right] \\
	 & = \fracN \fracM \left[ N \sumNj x_j - \sumNi x_i \right]              \\
	 & = \fracN \fracM (N-1) \sumNi x_i                                      \\
	 & = \fracN \sumNi x_i                                                   \\
	 & = \hat{\mu}                                                           \\
\end{align*}

\section*{Problem 3}

\newcommand{\normsq}[1]{\Vert #1 \Vert^2}

For each clustering, we compute the sum of squared error $J_e$.

\begin{enumerate}
	\item We first compute the centroid of each cluster.

	      \begin{align*}
		      m_1 & =
		      \frac{1}{2} (x_1 + x_2) =
		      \frac{1}{2} \left(
		      \begin{bmatrix} 0 \\ 0 \end{bmatrix} +
		      \begin{bmatrix} 1 \\ 1 \end{bmatrix}
		      \right)                  =
		      \begin{bmatrix} 0.5 \\ 0.5 \end{bmatrix} \\
		      m_2 & =
		      \frac{1}{2} (x_3 + x_4) =
		      \frac{1}{2} \left(
		      \begin{bmatrix} 1 \\ 0 \end{bmatrix} +
		      \begin{bmatrix} 2 \\ 0.5 \end{bmatrix}
		      \right)                 =
		      \begin{bmatrix} 1.5 \\ 0.25 \end{bmatrix}
	      \end{align*}

	      And then the squared error for each point.

	      \begin{align*}
		      \normsq{x_1 - m_1} = (0-0.5)^2 + (0-0.5)^2 = 0.5       \\
		      \normsq{x_2 - m_1} = (1-0.5)^2 + (1-0.5)^2 = 0.5       \\
		      \normsq{x_3 - m_2} = (1-1.5)^2 + (0-0.25)^2 = 0.3125   \\
		      \normsq{x_4 - m_2} = (2-1.5)^2 + (0.5-0.25)^2 = 0.3125 \\
	      \end{align*}

	      And finally, the we compute $J_e$ as the sum of all errors.

	      \begin{align*}
		      J_e^{(1)} = 0.5 + 0.5 + 0.3125 + 0.3125 = 1.625
	      \end{align*}

	\item We repeat the same process for the second clustering. Centroids first,

	      \begin{align*}
		      m_1 & =
		      \frac{1}{2} (x_1 + x_4) =
		      \frac{1}{2} \left(
		      \begin{bmatrix} 0 \\ 0 \end{bmatrix} +
		      \begin{bmatrix} 2 \\ 0.5 \end{bmatrix}
		      \right)                  =
		      \begin{bmatrix} 1 \\ 0.25 \end{bmatrix} \\
		      m_2 & =
		      \frac{1}{2} (x_2 + x_3) =
		      \frac{1}{2} \left(
		      \begin{bmatrix} 1 \\ 1 \end{bmatrix} +
		      \begin{bmatrix} 1 \\ 0 \end{bmatrix}
		      \right)                 =
		      \begin{bmatrix} 1 \\ 0.5 \end{bmatrix}
	      \end{align*}

	      Then individual errors,

	      \begin{align*}
		      \normsq{x_1 - m_1} = (0-1)^2 + (0-0.25)^2 = 1.0625   \\
		      \normsq{x_4 - m_1} = (2-1)^2 + (0.5-0.25)^2 = 1.0625 \\
		      \normsq{x_2 - m_2} = (1-1)^2 + (1-0.5)^2 = 0.25      \\
		      \normsq{x_3 - m_2} = (1-1)^2 + (0-0.5)^2 = 0.25      \\
	      \end{align*}

	      And sum of errors

	      \begin{align*}
		      J_e^{(2)} = 1.0625 + 1.0625 + 0.25 + 0.25 = 2.625
	      \end{align*}

	\item We repeat the same process for the third clustering. Centroids first,

	      \begin{align*}
		      m_1 & =
		      \frac{1}{3} (x_1 + x_2 + x_3) =
		      \frac{1}{3} \left(
		      \begin{bmatrix} 0 \\ 0 \end{bmatrix} +
		      \begin{bmatrix} 1 \\ 1 \end{bmatrix} +
		      \begin{bmatrix} 1 \\ 0 \end{bmatrix}
		      \right)                  =
		      \begin{bmatrix} \frac{2}{3} \\ \frac{1}{3} \end{bmatrix} \\
		      m_2 & =
		      \frac{1}{1} (x_4) =
		      \begin{bmatrix} 2 \\ 0.5 \end{bmatrix}
	      \end{align*}

	      Then individual errors,

	      \begin{align*}
		      \normsq{x_1 - m_1} = (0-\frac{2}{3})^2 + (0-\frac{1}{3})^2 = \frac{5}{9} \\
		      \normsq{x_2 - m_1} = (1-\frac{2}{3})^2 + (1-\frac{1}{3})^2 = \frac{5}{9} \\
		      \normsq{x_3 - m_1} = (1-\frac{2}{3})^2 + (0-\frac{1}{3})^2 = \frac{2}{9} \\
		      \normsq{x_4 - m_2} = (2-2)^2 + (0.5-0.5)^2 = 0                           \\
	      \end{align*}

	      And sum of errors

	      \begin{align*}
		      J_e^{(3)} = \frac{5}{9} + \frac{5}{9} + \frac{5}{9} + 0 = \frac{5}{3}
	      \end{align*}
\end{enumerate}
Since $\frac{5}{3} < 1.625 < 2.625$, clustering 3. is the clustering minimizing $J_e$.

\section*{Problem 4}
\begin{enumerate}[a)]
	\item First of all, we assign each sample to the cluster having the closest mean, which leads to the following initial partitioning.

	      \begin{align*}
		      C_1 & = \{ x_3, x_4, x_6, x_7\} \\
		      C_2 & = \{ x_1, x_2, x_5 \}
	      \end{align*}

	      We then follow the algorithm by taking the random samples in the given order.

	      \begin{enumerate}[1.]
		      \item Sample chosen : $x_1$
		            \begin{align*}
			            \rho_1 & = \frac{4}{5}\normsq{x_1-m_1} = 4 \\
			            \rho_2 & = \frac{3}{2}\normsq{x_1-m_2} = 6 \\
			            \rho_1 & < \rho_2 \implies \text{transfer} \\
		            \end{align*}
		            so we transfer $x_1$ from $C_2$ to $C_1$
		            \begin{align*}
			            m_1^* & = m_1 + \frac{x_1-m_1}{5} = \begin{bmatrix} \frac{9}{5} & \frac{13}{5}\end{bmatrix}^T \\
			            m_2^* & = m_2 - \frac{x_1-m_2}{2} = \begin{bmatrix} 2 & 1\end{bmatrix}^T \\
			            C_1   & = \{ x_1, x_3, x_4, x_6, x_7\}                           \\
			            C_2   & = \{ x_2, x_5 \}                                         \\
		            \end{align*}

		      \item Sample chosen : $x_2$
		            \begin{align*}
			            \rho_1 = \frac{5}{6}\normsq{x_2-m_1} = \frac{10}{3} \\
			            \rho_2 = \frac{2}{1}\normsq{x_2-m_2} = 2            \\
			            \rho_1 > \rho_2 \implies \text{no transfer}         \\
		            \end{align*}

		      \item Sample chosen : $x_3$
		            \begin{align*}
			            \rho_1 = \frac{5}{6}\normsq{x_3-m_1} = \frac{1}{6} \\
			            \rho_2 = \frac{2}{1}\normsq{x_3-m_2} = 8           \\
			            \rho_2 > \rho_1 \implies \text{no transfer}        \\
		            \end{align*}

		      \item Sample chosen : $x_7$
		            \begin{align*}
			            \rho_1 = \frac{5}{6}\normsq{x_7-m_1} = \frac{17}{6} \\
			            \rho_2 = \frac{2}{1}\normsq{x_7-m_2} = 20           \\
			            \rho_2 > \rho_1 \implies \text{no transfer}         \\
		            \end{align*}

		      \item Sample chosen : $x_6$
		            \begin{align*}
			            \rho_1 = \frac{5}{6}\normsq{x_6-m_1} = \frac{4}{3} \\
			            \rho_2 = \frac{2}{1}\normsq{x_6-m_2} = 10          \\
			            \rho_2 > \rho_1 \implies \text{no transfer}        \\
		            \end{align*}

		      \item Sample chosen : $x_4$
		            \begin{align*}
			            \rho_1 = \frac{5}{6}\normsq{x_4-m_1} = \frac{85}{24} \\
			            \rho_2 = \frac{2}{1}\normsq{x_4-m_2} = 25            \\
			            \rho_2 > \rho_1 \implies \text{no transfer}          \\
		            \end{align*}

		      \item Sample chosen : $x_5$
		            \begin{align*}
			            \rho_1 = \frac{5}{6}\normsq{x_5-m_1} = \frac{17}{12} \\
			            \rho_2 = \frac{2}{1}\normsq{x_5-m_2} = 1             \\
			            \rho_2 < \rho_1 \implies \text{transfer}             \\
		            \end{align*}
		            so we transfer $x_5$ from $C_2$ to $C_1$
		            \begin{align*}
			            m_1^* & = m_1 + \frac{x_5-m_1}{6} = \begin{bmatrix} \frac{23}{12} & \frac{29}{12}\end{bmatrix}^T \\
			            m_2^* & = m_2 - \frac{x_5-m_2}{1} = \begin{bmatrix} \frac{3}{2} & \frac{1}{2}\end{bmatrix}^T \\
			            C_1   & = \{ x_1, x_2, x_3, x_4, x_6, x_7\}                      \\
			            C_2   & = \{ x_2 \}                                              \\
		            \end{align*}
	      \end{enumerate}

	      The algorithm terminates here because we cannot compute $\rho_2$ as it would lead to a division by zero. We get a final clustering of

	      \begin{align*}
		      C_1 & = \{ x_1, x_2, x_3, x_4, x_6, x_7\} \\
		      C_2 & = \{ x_2 \}
	      \end{align*}

	      which is obviously an incorrect result. The reason this is incorrect is because we initially considered $m_1$ and $m_2$ to be the cluster means, when in reality they should have been recomputed according the initial partitioning.

	\item The criterion function used is the sum of the squared errors over all clusters, i.e.

	      \[
		      J = \sum_{i=1}^M \sum_{x \in X_i} \normsq{x-m_i} \\
	      \]
	      with
	      \[
		      m_i = \fracN \sum_{x \in X_i} x
	      \]

	      For each random sample, we only apply a transfer if the increase of the criterion function from the new cluster is lower than the decrease from the old cluster. This leads to all transfers resulting in a net decrease of the global criterion function. Since the criterion function is a positive function which is strictly decreasing, and the number of different clustering is finite ($N!$), the algorithm is guaranteed to converge.
\end{enumerate}

\end{document}
